{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's say that we were investigating the behavior of users on a social networking site (referenced in chp 1).  We believe that having more friends causes people to spend more time on the site.  We're asked to build a model to describe this relationship.  We hypothesize that there are constants $\\alpha$ and $\\beta$ such that:\n",
    "\n",
    "$$ y_i = \\beta x_i + \\alpha + \\epsilon _i$$\n",
    "\n",
    "where $y_i$ is the number of minutes user $i$ spends on the site daily, $x_i$ is the number of friends user $i$ has, and $\\epsilon _i$ is a error term representing the fact that there are other factors not accounted for by such a simple model.\n",
    "\n",
    "Here's what the model looks like as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from code_python3.linear_algebra import vector_subtract\n",
    "from code_python3.stats import mean, correlation, standard_deviation, de_mean\n",
    "from code_python3.gradient_descent import minimize_stochastic\n",
    "import math, random\n",
    "\n",
    "def predict(alpha, beta, x_i):\n",
    "    return beta * x_i + alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know the output of `y_i` we can compute the error for each pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(alpha, beta, x_i, y_i):\n",
    "    return y_i - predict(alpha, beta, x_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to know the total error over the entire data set.  But we don't just add the errors.  If the prediction for `x_1` is too high and the prediction for `x_2` is too low, the errors may just cancel out.  Instead, we add the squared errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squared_errors(alpha, beta, x, y):\n",
    "    return sum(error(alpha, beta, x_i, y_i) ** 2 for x_i, y_i in zip(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *least squares solution* is to choose the `alpha` and `beta` that make `sum_of_squared_errors` as small as possible.  Using calculus, the error minimizing alpha and beta are given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit(x,y):\n",
    "    \"\"\"given training values for x and y,\n",
    "    find the least-squares values of alpha and beta\"\"\"\n",
    "    beta = correlation(x, y) * standard_deviation(y) / standard_deviation(x)\n",
    "    alpha = mean(y) - beta * mean(x)\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of `alpha` simply says that when we see the average value of the independent variable `x`, we predict the average value of the dependent variable `y`.  The choice of `beta` means that when the input value increases by `standard_deviation(x)`, the prediction increaes by `correlation(x, y) * standard_deviation(y)`.  In the case when `x` and `y` are perfectly correlated, a one standard deviation increase in `x` results in a one standard deviation of `y` increase in the prediction.  When they are perfectly anti-correlated, the increase in `x` results in a decrease in the prediction.  And when the correlation is zero, `beta` is zero, which means that changes in `x` don't affect the prediction at all: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 22.94755241346903; beta = 0.903865945605865\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3X2cVdV97/HPj/GgAzEOGLQygiTWoiFUiBOxL9JeJQ+kxoe5iUlKTEp600tzb29fSZPQQF/eCr3kSkJvHtqmD7RpQ67WZx0xpkVfimljE8wgGqTCNRoRBxUSGJ8Y4wC/+8feB88c9trn7MM5c/ac832/Xrxmzj4Pe+2Z4XfWWeu3fsvcHRERGfvGNbsBIiJSHwroIiItQgFdRKRFKKCLiLQIBXQRkRahgC4i0iIU0McIM7vSzO5u0Gt/y8xW1fk1G9bedmNmv25mO5rdjmqZ2SfM7PtVPrbuf3vtTAE9R8zsnWb272b2gpntM7MHzOwdAO5+nbu/t9ltLGdmbma/XH48T+2Ng8ZrZvZy/HO9x8zObna7quXu/+buM+v9umY2I/79PVR2/E3xz+upep9TGksBPSfM7I3Ad4C/ACYD3cBK4BfNbNdYY2bHBe76sru/gejnOgB8c5TPn2cTzextJbc/Cvy0WY2R2img58evALj79e5+yN2H3P1ud/8xHP0xNu5Z/Xcze9zMXjKz/2VmZ5rZD8zsRTO7yczGx4+90MyeMbM/NrOfmdlTZnZlqCFmdomZPWxmg/Enhl/NejGB9n4qbu9+M/uGmVnJ/f/FzB6L79tgZmeU3Pd1M9sVX9dmM/v1kvtWmNktZnatmb0IfCKtXe4+BNwEzClrb9r532tmO+JPTn9lZt8zs98tuc4HzOyrZrYPWJH2ehb5qpntiV/vx8VgamYXm9l/xL/PATP7fHz8QjN7pqQ955jZ/fHvZ5uZXVZy37fin+1d8etsMrMzK/y6/i+wuOT2bwPfLvv5pJ3zZDNbH/9+HgTOLHvu2fGnon3xz/HDFdojtXJ3/cvBP+CNwM+BdcBvApPK7v8E8P2S2w6sj583i6gnfy/wFuAk4D+AxfFjLwQOAl8Bjgf+E/AKMDO+/1vAqvj7twN7gHlAB9F/9KeA4wPtduCXE44ntfc7QBcwHdgLvC++rxf4CXAOcBxwFfDvJc/9GHByfN/ngOeAE+L7VgDD8WuMAzoT2lJ6fROJAtgjJfcHzw+8CXgR+EB836fj8/1uyXUeBP4gvr+zwustBDbHPweLH3NafN+zwK/H308C3l7y+3sm/r4Qv/YfA+OBBcBLZb/LfcD58bmvA24I/O5mxL+XGcCu+Pd9DrADeDfwVJXnvIHoTXIi8DaiT0DfL/l57wJ+J27P24GfAbPKfzf6d+z/1EPPCXd/EXgn0X+wvwP2xr2eU1Oe9iV3f9HdtwGPAne7+5Pu/gLwz8Dcssf/T3f/hbt/D7gLSOop/Vfgb919k0efFNYRvVlccGxXCMBqdx9096eBjbzeS/494Bp3f8zdDwL/G5hT7NW6+7Xu/nN3P+ju/4foTal0TPkH7t7n7oc96oEn+byZDRIFoncCHy+5L+38FwPb3P22+L4/J3pDKbXb3f8ibt9QhdcbBk4EzgYsfsyz8esMA281sze6+353f4ijXQC8If5Zvubu9xG9US4qecxt7v5gfO7rKPs0kuAZXg/iiynrnaed08w6gA8Cf+Lur7j7o0SdkqJLiN4Y/jH++TwE3ApcUaFNUgMF9ByJ/3N/wt1PJ+rpTAW+lvKU50u+H0q4/YaS2/vd/ZWS2zvj1y93BvC5+KP1YBwEpwUem1VpIDxQ0r4zgK+XnG8fUe+1G8DMPhcPX7wQ338SUc+5aFcV5/4zd+8i6o0OMfINIe38U0tf392dKACWKj9/8PXiYPiXwDeA581srUXzJxAFxouBnfGwzq8lXMdUYJe7Hy45tjNua1Ho55zm20SfNhYB12Y45xSinveusvuKzgDmlf09XQn8UhVtkowU0HPK3bcTfRx9W4WHVmuSmU0suT0d2J3wuF3AF929q+TfBHe/vk7tSLIL+L2yc3a6+7/H4+VfIPo0MSkOyi8QBciiqkuGxp8OPk0UcDsrnZ9oGOT04vPjcf/Ty1+22uuJ2/Dn7n4e0VDZrwBL4+M/cvfLgVOAPqJhjHK7gWlmVvp/dzrRMMexuBV4P/Cku+8suy/tnHuJhpymld1XtAv4XtnP4g3u/t+Osb2SQAE9J+KJo8+Z2enx7WlEvaUf1vE0K81sfBwkLwFuTnjM3wGfMrN58QTeRDN7v5mdmPK6483shJJ/HRnb9TfAcjObBWBmJ5nZh+L7TiQKGHuB48zsT4jmDWrm7vcQBaklVZz/LmC2mfValMHy+1TuXQZfz8zeEf9sC0TzGK8Ch+Lfy5VmdpK7DxON2x9KeO1N8fP+yMwKZnYhcCnROHbN4k9vC4DfzXJOdz8E3AasMLMJZvZWRk6wfgf4FTP7ePzcQvwzOOdY2ivJFNDz4yWiichNZvYKUSB/lGgSsB6eA/YTBbLrgE/FnwJGcPd+onH0v4wf/xMqZI4A24iGMYr/fidLw9z9duBLwA0WZao8SjQxDLCBaD7g/xF9lH+V6oZYKllDFKCOTzu/u/8M+BDwZaJJ67cC/aSkk1a4njcSvWnuj6/n58Cfxfd9HHgqfs6niCaDy1/7NeCy+PV+BvwV8NtJv8us3L3f3Z+o4Zz/g2hY5zmiT5X/WPLcl4D3Ar9F9Lf3HNHP5vhjba8czaIhQWllcY/q2nhsXo5BPOzwDHClu29sdntESqmHLlKBmS00sy4zO54odc+o71CYSF0ooItU9mvAE0TDDZcCvSnpkSJNoyEXEZEWoR66iEiLGNVCQm9605t8xowZo3lKEZExb/PmzT9z9ymVHjeqAX3GjBn09/eP5ilFRMY8Mytf7JVIQy4iIi2iYkA3s5kWlVIt/nvRzD5jZpPjkpiPx18njUaDRUQkWcWA7u473H2Ou88BziMq9nM7sAy4193PIirbuqyhLRURkVRZh1zeBTwRF++5nNfLZK4jqgEtIiJNkjWg/xZQrLp3arGOc/z1lKQnmNkSM+s3s/69e/fW3lIREUlVdZaLRduZXQYsz3ICd18LrAXo6enRKiYRaRt9WwZYs2EHuweHmNrVydKFM+md2135iTXKkrb4m8BD7l7cROF5MzvN3Z81s9OIti0TERGiYL78tq0MDUdVkAcGh1h+21aAhgX1LEMui3h9uAWi/SyLdY8XA3fUq1EiImPdmg07jgTzoqHhQ6zZsKNh56wqoJvZBOA9RIXsi1YD7zGzx+P7Vte/eSIiY9PuweT6baHj9VDVkIu7HyDadb302M+Jsl5ERKTM1K5OBhKC99SuzoRH14dWioqINMDShTPpLIzcjbGz0MHShTMDzzh2o1rLRUSkXRQnPvOa5SIiIhn0zu1uaAAvpyEXEZEWoYAuItIiFNBFRFqEArqISItQQBcRaREK6CIiLUIBXUSkRSigi4i0CAV0EZEWoYAuItIiFNBFRFqEArqISItQQBcRaREK6CIiLUIBXUSkRSigi4i0CAV0EZEWoYAuItIiFNBFRFqEArqISItQQBcRaRFVBXQz6zKzW8xsu5k9Zma/ZmaTzeweM3s8/jqp0Y0VEZGwanvoXwf+xd3PBs4FHgOWAfe6+1nAvfFtERFpkooB3czeCPwG8E0Ad3/N3QeBy4F18cPWAb2NaqSIiFRWTQ/9LcBe4B/NbIuZ/b2ZTQROdfdnAeKvpyQ92cyWmFm/mfXv3bu3bg0XEZGRqgnoxwFvB/7a3ecCr5BheMXd17p7j7v3TJkypcZmiohIJdUE9GeAZ9x9U3z7FqIA/7yZnQYQf93TmCaKiEg1KgZ0d38O2GVmM+ND7wL+A1gPLI6PLQbuaEgLRUSkKsdV+bg/AK4zs/HAk8DvEL0Z3GRmnwSeBj7UmCaKiEg1qgro7v4w0JNw17vq2xwREalVtT10kVHXt2WANRt2sHtwiKldnSxdOJPeud3NbpZIbimgSy71bRlg+W1bGRo+BMDA4BDLb9sKoKAuEqBaLpJLazbsOBLMi4aGD7Fmw44mtUgk/xTQJZd2Dw5lOi4iCuiSU1O7OjMdFxEFdMmppQtn0lnoGHGss9DB0oUzA88QEU2KSi4VJz6V5SJSPQV0ya3eud0K4CIZaMhFRKRFKKCLiLQIBXQRkRahMXQJ0tJ7kbFFAV0Saem9yNijIRdJpKX3ImOPArok0tJ7kbFHAV0Saem9yNijgC6JtPReZOzRpKgk0tJ7aTRlUdWfAroEaem9NIqyqBpDAV1GhXpjUioti0p/F7VTQJeGU29MylXKolIHoDaaFJWGU067lEvLoip2AAYGh3Be7wD0bRkY3UaOQQro0nDKaZdyaVlU6gDUTgFdGk457VKud24313xgNt1dnRjQ3dXJNR+YTe/cbnUAjkFVY+hm9hTwEnAIOOjuPWY2GbgRmAE8BXzY3fc3ppkyli1dOHPEGDoop13CWVRTuzoZSAje6gBUlqWHfpG7z3H3nvj2MuBedz8LuDe+LXKUtN6YSDktaqvdsWS5XA5cGH+/Drgf+MIxtkdalHLapVpa1FY7c/fKDzL7KbAfcOBv3X2tmQ26e1fJY/a7+6SE5y4BlgBMnz79vJ07d9at8VIfShETyTcz21wyOhJUbQ99vrvvNrNTgHvMbHu1DXH3tcBagJ6ensrvHnJMsgZn5YiLtI6qxtDdfXf8dQ9wO3A+8LyZnQYQf93TqEZKdWrJ31WKmEjrqBjQzWyimZ1Y/B54L/AosB5YHD9sMXBHoxop1aklOCtFTKR1VDPkcipwu5kVH/9P7v4vZvYj4CYz+yTwNPChxjVTqlFLcFaKmEjrqBjQ3f1J4NyE4z8H3tWIRkltagnOSxfOZOktjzB86PXpjUKHKUVMZAzSStEWUnP+bvlUtaauRcYkVVscRaEMlHqlDdaSv7tmww6GD4+M4MOHXWVMRcYgBfRREkoP7N+5j1s3DzQtbVCToiKtQ0MuoySUgXL9pl11SxusJW1RhbNEWocC+igJ9XgPBVbq1tJDriVtUXUzRFqHAvooCfV4O6J00Kofn6aW4RMVzhJpHRpDHyWhErIfPK97xBh68XgtPeRac8pVOEukNaiHPkpCPeFVvbPr1kPW8IlIe6uq2mK99PT0eH9//6idr5HyWqEwr+0SkdrVu9qilMhzhcJWGj7Rm5NINgroFSQFlbRsEgWc+sjzm6ZIXimgk76CMymolAfzIi3GqR+9aYpk1/YBPa0nGAoqHWaJ+eNajFM/WsEqkl3bZ7mk9QTTFgMpm6SxtIJVJLu2D+hpPcFQ8CimFmoxTuMoBVMku7YfcklbjBNaDFQcY1cAbxzt/C6SXdsH9EpBGxRUmkVvmiLZtH1AV9AWkVbR9gEdwj1B5UKLyFiigJ5CudD1o1WfIo2ngJ6i1XKhmxVU9UlHZHS0fdpimlbKha5lN6N6qWXjDRHJTj30FBedPYVrf/h04vFaXNW3les37eKQOx1mLJo3jVW9s0el59zM4aNW+6QjkldVB3Qz6wD6gQF3v8TM3gzcAEwGHgI+7u6vNaaZzbFx+95Mx9Nc1bd1xJvDIXeu/eHT/HTvyzz09AsNH45oZlCtdeMNEckmy5DLp4HHSm5/Cfiqu58F7Ac+Wc+G5UFSEEo7nub6TbsSjz/wxL5RGY5o5vCRVn2KjI6qArqZnQ68H/j7+LYBC4Bb4oesA3ob0cBmGpe83WfweJrQZtAhtbxppGlmUNW+pSKjo9ohl68BfwScGN8+GRh094Px7WeAxP+dZrYEWAIwffr02lvaBIcDMTh0PE2oQmPa4+up2QuotOpTpPEqBnQzuwTY4+6bzezC4uGEhyZGK3dfC6yFaAu6Gts55i2aNy1xgjUka4++GmkLqJQjLjL2VdNDnw9cZmYXAycAbyTqsXeZ2XFxL/10YHfjmplNHgPUqt7ZAEdluWzcvjdxeKV7lCYMlSMu0joqBnR3Xw4sB4h76J939yvN7GbgCqJMl8XAHQ1sZ9XqGaAmTSiw/8Bw4vG084feTFb1zj4S2EPthdGdMGz31bB5fPMXqdWxLCz6AvBZM/sJ0Zj6N+vTpGNTz0UsV186i0LHyNGlQodx9aWzEh/ft2WApbc8MmLxztJbHkldvNPsCcN2zhFv5mIrkUbItLDI3e8H7o+/fxI4v/5NOjb1DFBZJxJX3rmN4UMjx76HDzkr79yWGqCbOWHYzjni7f7pRFpPy60UTQtQtXy8zhJsk4Zn0o7nQVo9+FbXzp9OpDW1XC2XUL71RWdP0cfrBM0e8mmmVqrVIwIt2EMPDZNUGluvx8SYGSRlGxZTyps9ARc6f7vmiLfzpxNpTS0X0CF5mOQzNz6c+NhiT70eWTGh1HH39OwbaPyCH6UnHq3Zi61E6s28AQtYQnp6ery/v3/UzldqxrK7Mj2+u6uTB5YtyNSrnvundwfTHCeMPy5xbH/ShAKvDh8+qpdY72GP+avvC+a7P7BsQd3OIyL1Z2ab3b2n0uNabgy9XnYPDmVOQ3y1bEin9Hhoom3/geFRKc6lCUCR1qeAHjC1qzM1DTHJ0PDh4PGsE221Btq+LQPMX30fb152F/NX33fkzUcTgCKtb8yOoYc2i6hFZ6EjcWIsNO6+/8Bw4lBMmtAE3PHHjWNw6OhhmloCbdo4uSYARVrfmOyhFzeLKBawKm4WcVXf1grPTFZL2t7Sm8uGYm5+hAmF5B/npAkFeud288Hzuo9UUeww44PndbPislk1lbVN6olXWijTrumJIu1iTE6Knrn8u4nVCDvMeOKaixOf8+ZldyWWgzTgp6vfn/icrBOpEwrjGD7sI4ZpCh3GmivOBUjsIV/zgehTRZZMi1D9l/JgXpR2jfXW7NRMkVZU7aTomBxyCZWWTSs5e+UF0xPL1155Qf1qtB8YPszXPjInMaDNX31fsPf8wLIFmYJeqCceqrk+WuPkSo0Uaa4xGdBrESpfW+u4e0hokU49s0xCzznkHpwPGA2qjSLSXG0T0CG5fG09pZXVrWeNmdBrdZesim3GkIdSI0WaK/cBPSmbpTsloEH9xnFDQxjjDDrG2VFj5aGyuhDOMinWmMkyTJGWsaLKjSLtK9dZLqFslhkndwYzQ/q2DCRmoKQV4bqqbytnLv8uM5bdxZnLv3skW2bRvGmJj//ovOmsueLcERkja644t2KJ3KQsk43b92ZeWBTKmGn2sEYzN6IWkZz30P9pU/IenD94ch9f+XDy5OOclXczXLaL8/BhZ8X6aDFQ+XP6d+4bMVlafNOAyuPuWQNoUu/5DwO57mnDFH1bBrh188CIN7pbNw/Qc8bkpgZ11UYRaa5cB/TDgaSVwx6efExapFM8njS0EVquf/2mXazqnU3PGZPZuH0vuweH+KWTTqDnjMm1XUxALcMUeZ58bNfKjSJ5kOuAXm9JQTDkkHvdKyQmje3XsoJTk48ikiTXY+iW8Xg9dZgFe8Ir79yWeZw+tH8lZF+pqrosIpIk1wE9tEyonmtbJ47vSDy+aN601AqJaeP0Seq5ebUmH0UkSa6HXCZNKATri4dSE4O7BgEnJCy6+eJ/nk3/zn2JE58bt+9NHN8OCY3fQ3g4pNi7L75BFG9DeNI1z5OPWvov0jy5ruUyZ+XdiUEyrWZKqEIiEFyWH5K1ZkraOUIbTITegLo6Czx89XuD58mj0M9LRcBEjk1L1HIJ9XgPJNQdL9YpT1t0lDUDI9QTXnnntsRPDhPHdwQnUS86e0piLZnQ+2labz+v8px9I9IOch3Qs9p/YJirL51V17rfoTeBpbc8ctQnhELH0bXNG7H7UF4p+0akuSpOiprZCWb2oJk9YmbbzGxlfPzNZrbJzB43sxvNbHzjm1vZaNT97p3bnbhS9IVAr3r34FCmsXiIyguMNcq+EWmuanrovwAWuPvLZlYAvm9m/wx8Fviqu99gZn8DfBL463o2LlRLpZJQrzo0YVfLRF7SOVas35Y4VHJSZ4GXXj2Y6VpCi6ryTLsiiTRXxYDu0azpy/HNQvzPgQXAR+Pj64AV1DmgL5o3LXHcuTAOkrbvTKt2GFok1L9zH7duHqhLDW8L9KrN0mu1J+keg73aPGffiLSDqsbQzawD2Az8MvAN4Alg0N0Pxg95Bkj8X2tmS4AlANOnZ9tMIlRLpeeMyYlj2MVqh0kVGkNFsIqPKz9ey0TeYMJEafF4KAVz4vgODjst06vV0n+R5qkqoLv7IWCOmXUBtwPnJD0s8Ny1wFqI0hazNjCthnlST7BYobGotNhWklDPudJEXtIwTVcgaHdNKASzWQod41hx2Sz1akXkmGXKcnH3QTO7H7gA6DKz4+Je+unA7ga0L7PrN+2qy+ukTeSFa7wkR213ghOmLwwNq1crInVRMaCb2RRgOA7mncC7gS8BG4ErgBuAxcAdjWxoqbSiWbVMombdti2Ubx0yODQczI+vdwaIVmqKtK9qarmcBmw0sx8DPwLucffvAF8APmtmPwFOBr7ZiAb2bRlg/ur7ePOyu5i/+r4jASu0gKUjNDMZMHF8R+Y0x6x51R1mLF04k0LHyLYVOqyuY+WhAmBpRcNEpHVUk+XyY2BuwvEngfMb0aiivi0DIyY/BwaHjpoMLbV7cIgrL5ieOmZe7pXXDtG/cx/PvfAqDjz3wqv079xX056eIUc+NZQ3u86pibWu1FSvXqQ15Lra4so7tx0VvIcPeTA9cGpXJ6t6ZzP/zJGbUJTfLpe0zV1xG7okoWqHXZ3JaZPdXZ2s2bAjsULjmg07Ej+F1KKWlZrq1Yu0jlwH9KSMEQjXP7no7Cn0bRngwaf2jzhefrsa1wW2v4PwatQVl80KlrVNq7ZYr4Bay0rNepb1FZHmaqlaLhu37+WuHz+b2KvPqtLcalpmStLwRWgVqZG8k1ItefDa/UikveU6oBvZhpl3x73ckHE2ckl9+e0sQuPOoUAfGiYKnb6WgFrLSs1a9jQVkXzKdUDPGmsrTVZ2mHG4pOtdfjtJUuAGgmmToeAZGj5Ku5Z61ZhJo/orIq0j12PooXomkyYUgmPVoYlJg8RJyTShCcMV67dlHncOpVOGkixnnNw5KpOVo1GdUkRGR6576KHeY7FmS1LvtX/nvuSNJDKeu5iZkhS4Q4uI0j4dhBY8hdr1wyf3163GTCVaqSrSGnId0CuNCScFoY3b9ya+Vmi8fOL4Dl47eHhEb70wLlrw84cp29mFzhESWikaUmuNGRFpX7kO6JC99xgKeKHRFXc/etwjvp11AVHaCE5oC7rOwjiGEmoBh2rBa7JSREJyPYYOyUv/045nDXgHhg8npjmu2bCDi86eUp+LIPzJ4YRCR+J8wKJ504LzBCIiSXLdQ69lU4pQTzi0KUbIwOBQMAiHGOF0xtAnh8EDw1x5wfQR9ds/eF43q3pn03PGZC3JF5GqmddQnbBWPT093t/fX/Xj56++L3HIIzQcUcyKSXpO1pzzYkpj1p9OUuXGaz4wmzUbdiS2a9KEAq8OH058joK3iACY2WZ376n0uFwPuYTGr9MmDLOOoYcccq9pvDqUzrh04UwKZbOmhXGGe/g5IiJZ5DqgZ6yEy9SuzmAQDuWBh453x0McSePYWR15k0mYfE0qBwDpKZAiIklyHdDTRoNCE4ahIByaZEybfAwtugkteAq9OUwtVlusQ40ZEZGQXE+KpimOS4cmDJPuC00ypk0+htImkxY8ffC87hGTtcXjteS0i4hkNWYDelp+eui+rMfTzg3hBU/lGSu9c7uDk6IhWXdeEhHJdUAPVVusFOpCqYP13Jkn6U2gb8sAt24eGLFZxq2bB+g5Y3IwnfKsUyby+J5Xjjq+aN60mtolIu0r1wE9NMKcNvJcS+46ZCs5G1LLZhEHXjvM/DMn88AT+44cm3/mZFb1zs58fhFpb7kO6JMmFBLLzk6akFxREcJBtTgMUn585Z3bRuSBV1MKN6SWzSIGBofY98prI4499PQL9G0ZUB66iGQyJrNc0rJfQsEzlLu+/8Bw3fLA07aA6wq8CZkpD11E6iPXAf2FQI526DiEg2rWScZaqhqGar9cdPaUzG9OqqooIlnlOqCn9XhDxbmy5qFPKCT/CEI96jSh2i8bt+9NfRNKoqqKIpJVrgN6KDhfdPYUlt78yIjdfJbe/MiRceekxUCremcnHg+N3rwa2MQiTdoYeihATxyfvPK0npUeRaQ9VJwUNbNpwLeBXwIOA2vd/etmNhm4EZgBPAV82N3317NxoXzvFeu3JW4nt2L9ttSNmpOOfyaw4CepRnmppBTItA2XQ7svFTrGAUe/eWSt9CgiUk2Wy0Hgc+7+kJmdCGw2s3uATwD3uvtqM1sGLAO+UO8GZgnCobooxyLLJtFpK0VDb06hFaQaQxeRrCoGdHd/Fng2/v4lM3sM6AYuBy6MH7YOuJ8GBPR6SgrOaamRoZz2448bl5iZsnH73oolCcql9epFRLLIlIduZjOAucAm4NQ42OPuz5rZKYHnLAGWAEyfPv1Y2lq1q/q2jlh+v2jeNHrOmBzsVd/4o10jCmUVOoyrL51V0ybRoeGe0JtDWq9eRCSLqidFzewNwK3AZ9z9xWqf5+5r3b3H3XumTKnPRF9oYdGkCQWu6tvKtT98esTy+2t/+DTLb/txsFd9/oxJI46fP2NS6i5DIWmpkaE3h2KvvnyyVouKRCSrqnroZlYgCubXuftt8eHnzey0uHd+GrCnUY0sd/Wls1h6yyOJverP3pRtknNgcOioIY8HntjHVX1bM28SHVq8BOkZMFmLg4mIJKnYQzczA74JPObuXym5az2wOP5+MXBH/ZuXrHduNx95x7QjPeIOMz7yjmn0zu3OvDNRyPWbdkW7DHWU7TLUYXR1Jn9CCNVJBzgp8JzQcRGRrKoZcpkPfBxYYGYPx/8uBlYD7zGzx4H3xLdHRaiqYXFxUT0c6W2Xv0E4XHLuacFNMUJCozGqkisi9VJNlsv3CVesfVd9m3O0pMyUtKqGnYVxFXPIq9FhFu0ylJDvXks2y2BCJk3acRGRrHK9UrSYGVK6IrR4O8nuwSGu+cAsAMnFAAAIE0lEQVSvHnVRtVzkonnTaqqeGJJWxkBEpB5yHdBDPfG0vTt753bzlY/MGZE1UrydpLurk49dMH3EePzHLpjOqt7ZwWDbNaGQ+EaTNuQTKmOg9EQRqZdc10NPK4VbGGcjhkMK4+xIcMyyD2hxqCRpQ4nQcn33cMnbtG3xoD4baYiIJMl1QA+lDU6aUODlXxwcebDC5GItAbV3bjf9O/cdtUfodQlbyUHloRilJ4pII+U6oKf1kEtz0CG6ndZDhuwBtW/LADc+uGtENs2ND+7ipM5CYt0YjYeLSDPlegw9VAo3VFu83gWtQlUdXzt4SOPhIpI7ue6hQ3Kves2GHaNS0CpUvfHA8GG+9pE5icM3SWmWGmYRkdGQ+4CeFCBDQzHFHvJoBNWkN5pQAa7i40VEGinXQy6hPHQgWNAq9Jy+LQPBbetCxgUmWkPH0xY8iYg0Wq576GkB8oFlCxJ7vaHnrLxzG68OH87Uew7VhQkdr+dCJBGRrHLdQ68lQIbu239gOHPvOW0xUhKtBhWRZsp1QO8K1D0PHa90X5KBwaHgUExoo+bQca0GFZFmyvWQS6i8eErZcV5+NVuxq3GWvD8ohDdq3rh9b+rEq7JcRKQZch3QQ/nmLwwNBwNq1kKLh1OW8YeGb4pBPzQerwAuIs2Q6yGXehbHyqr4RpGkw0zZLCKSO7kO6KEx6bTiWKGUQoufW/5aod2Hir3+pOeEtppTNouINFOuA3otS/8/Om964n1XXjA98bVWXDYrOJEZOn/WLBcRkdGQ6zF0yL70v1gGt7RC4qJ5044cD41vhyYys5biFRFpltwH9CQXnT2FaxNK2BbTCVf1zk6sbx6SdSJT2SwikkdjMqCnpROOFmWziEje5HoMPURL7EVEjjYmA7qW2IuIHG1MBnQtsRcROVrFMXQz+wfgEmCPu78tPjYZuBGYATwFfNjd9zeigVpiLyJSHfO0wiiAmf0G8DLw7ZKA/mVgn7uvNrNlwCR3/0Klk/X09Hh/f3/VjevbMsDSmx8ZsQ1cYZyx5kPnKniLSNsws83u3lPpcRWHXNz9X4F9ZYcvB9bF368DejO3sAqhPT1XrN/WiNOJiIxptY6hn+ruzwLEX0+pX5NeF9rTM3RcRKSdNTwP3cyWAEsApk9PXpZfC23GLCIyUq099OfN7DSA+Oue0APdfa2797h7z5QpyRtDhEwKbFYxcXxHw6stioiMNbUG9PXA4vj7xcAd9WnOSFdfOuuo6onjDAod41S+VkSkTMWAbmbXAz8AZprZM2b2SWA18B4zexx4T3y7ITrKInrHOAuOoWulqIi0s4pj6O6+KHDXu+rclqOs2bCD4UNlWS6HogqKSTXJtVJURNpZrleKhnrch9y1UlREpEyuA3qox1260UTpxhPKchGRdpbr8rlLF84MbiSh8rUiIiPlOqCrZouISPVyHdBBG0mIiFQr12PoIiJSPQV0EZEWoYAuItIiFNBFRFqEArqISIuouGNRXU9mthfYWeFhbwJ+NgrNyaN2vnZo7+vXtbevaq7/DHevWK52VAN6Ncysv5qtllpRO187tPf169rb89qhvtevIRcRkRahgC4i0iLyGNDXNrsBTdTO1w7tff269vZVt+vP3Ri6iIjUJo89dBERqYECuohIi8hNQDez95nZDjP7iZkta3Z7Gs3M/sHM9pjZoyXHJpvZPWb2ePx1UjPb2ChmNs3MNprZY2a2zcw+HR9v+es3sxPM7EEzeyS+9pXx8Teb2ab42m80s/HNbmujmFmHmW0xs+/Et9vp2p8ys61m9rCZ9cfH6vZ3n4uAbmYdwDeA3wTeCiwys7c2t1UN9y3gfWXHlgH3uvtZwL3x7VZ0EPicu58DXAD8fvz7bofr/wWwwN3PBeYA7zOzC4AvAV+Nr30/8MkmtrHRPg08VnK7na4d4CJ3n1OSe163v/tcBHTgfOAn7v6ku78G3ABc3uQ2NZS7/yuwr+zw5cC6+Pt1QO+oNmqUuPuz7v5Q/P1LRP+5u2mD6/fIy/HNQvzPgQXALfHxlrx2ADM7HXg/8PfxbaNNrj1F3f7u8xLQu4FdJbefiY+1m1Pd/VmIgh5wSpPb03BmNgOYC2yiTa4/HnJ4GNgD3AM8AQy6+8H4Ia389/814I+Aw/Htk2mfa4fozftuM9tsZkviY3X7u8/LjkWWcEz5lC3OzN4A3Ap8xt1fjDprrc/dDwFzzKwLuB04J+lho9uqxjOzS4A97r7ZzC4sHk54aMtde4n57r7bzE4B7jGz7fV88bz00J8BppXcPh3Y3aS2NNPzZnYaQPx1T5Pb0zBmViAK5te5+23x4ba5fgB3HwTuJ5pH6DKzYgerVf/+5wOXmdlTRMOqC4h67O1w7QC4++746x6iN/PzqePffV4C+o+As+LZ7vHAbwHrm9ymZlgPLI6/Xwzc0cS2NEw8bvpN4DF3/0rJXS1//WY2Je6ZY2adwLuJ5hA2AlfED2vJa3f35e5+urvPIPo/fp+7X0kbXDuAmU00sxOL3wPvBR6ljn/3uVkpamYXE71bdwD/4O5fbHKTGsrMrgcuJCqd+TxwNdAH3ARMB54GPuTu5ROnY56ZvRP4N2Arr4+l/jHROHpLX7+Z/SrRxFcHUYfqJnf/UzN7C1GvdTKwBfiYu/+ieS1trHjI5fPufkm7XHt8nbfHN48D/sndv2hmJ1Onv/vcBHQRETk2eRlyERGRY6SALiLSIhTQRURahAK6iEiLUEAXEWkRCugiIi1CAV1EpEX8f8jmniygDf9dAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "num_friends_good = [49,41,40,25,21,21,19,19,18,18,16,15,15,15,15,14,14,13,13,13,13,12,12,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "daily_minutes_good = [68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,34.76,54.01,38.79,47.59,49.1,27.66,41.03,36.73,48.65,28.12,46.62,35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,36.32,20.45,21.93,26.02,27.34,23.49,46.94,30.5,33.8,24.23,21.4,27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,26.41,26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,36.31,21.03,30.86,36.07,28.66,29.08,37.28,15.28,24.17,22.31,30.17,25.53,19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,24.81,19.33,28.77,24.26,31.98,25.73,24.86,16.28,34.51,15.23,39.72,40.8,26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,35.59,39.43,14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,20.9,21.9,18.79,27.61,27.21,26.61,29.77,20.59,27.53,13.82,33.2,25,33.1,36.65,18.63,14.87,22.2,36.81,25.53,24.62,26.25,18.21,28.08,19.42,29.79,32.8,35.99,28.32,27.79,35.88,29.06,36.28,14.1,36.63,37.49,26.9,18.58,38.48,24.48,18.95,33.55,14.24,29.04,32.51,25.63,22.22,19,32.73,15.16,13.9,27.2,32.01,29.27,33,13.74,20.42,27.32,18.23,35.35,28.48,9.08,24.62,20.12,35.26,19.92,31.02,16.49,12.16,30.7,31.22,34.65,13.13,27.51,33.2,31.57,14.1,33.42,17.44,10.12,24.42,9.82,23.39,30.93,15.03,21.67,31.09,33.29,22.61,26.89,23.48,8.38,27.81,32.35,23.84]\n",
    "\n",
    "\n",
    "alpha, beta = least_squares_fit(num_friends_good, daily_minutes_good)\n",
    "print(\"alpha = {0}; beta = {1}\".format(alpha, beta))\n",
    "\n",
    "plt.scatter(num_friends_good, daily_minutes_good)\n",
    "plt.title(\"Simple Linear Regression Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model says that we expect a user with `n` friends to spend `22.95 + n * 0.903` minutes on the site each day.  That is, we predict that a user with no friends on the social media site would still spend about 23 minutes a day on the site.  And for each additional friend we expect a user to spend almost a minute more on the site each day.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to see how well we've fit the data.  A common measure is the *coefficient of determination* or R-squared, which measures the fraction of the total variation in the dependent variable that is captured by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3291078377836305"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def total_sum_of_squares(y):\n",
    "    \"\"\"the total squared variation of y_i's from their mean\"\"\"\n",
    "    return sum(v ** 2 for v in de_mean(y))\n",
    "\n",
    "def r_squared(alpha, beta, x, y):\n",
    "    \"\"\"the fraction of variation in y captured by the model, which equals\n",
    "    1 - the fraction of variation in y not captured by the model\"\"\"\n",
    "\n",
    "    return 1.0 - (sum_of_squared_errors(alpha, beta, x, y) / total_sum_of_squares(y))\n",
    "\n",
    "r_squared(alpha, beta, num_friends_good, daily_minutes_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we chose the `alpha` and `beta` that minimizes the sum of the squared prediction errors.  One linear model we could have chosen is \"always predict `mean(y)`\" corresponding to `alpha = mean(y)` and `beta = 0`, whose sum of squared errors exactly equals its total sum of squares.  This means an R-squared of zero, which indicates a model that performs no better than just predicting the mean.\n",
    "\n",
    "The least squares model must be at least as good as that one, which means the sum of the squared errors is at most the total sum of squares, which means that the R-squared must be at least zero.  And the sum of squared errors must be at least zero, which means that the R-squared can be at most one.\n",
    "\n",
    "The higher the number, the better our model fits the data.  Here we calculate an R-squared of 0.329, which tells us our model is only sort of okay at fitting the data, and that clearly there are other factors at play.\n",
    "\n",
    "If we write `theta = [alpha, beta]`, the we can also solve this using gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 22.93746417548679; beta = 0.9043371597664965\n"
     ]
    }
   ],
   "source": [
    "def squared_error(x_i, y_i, theta):\n",
    "    alpha, beta = theta\n",
    "    return error(alpha, beta, x_i, y_i) ** 2\n",
    "\n",
    "def squared_error_gradient(x_i, y_i, theta):\n",
    "    alpha, beta = theta\n",
    "    return [-2 * error(alpha, beta, x_i, y_i),       # alpha partial derivative\n",
    "            -2 * error(alpha, beta, x_i, y_i) * x_i] # beta partial derivative\n",
    "\n",
    "# choose random value to start\n",
    "random.seed(0)\n",
    "theta = [random.random(), random.random()]\n",
    "alpha, beta = minimize_stochastic(squared_error, squared_error_gradient, num_friends_good, \n",
    "                                  daily_minutes_good, theta, 0.0001)\n",
    "\n",
    "print(\"alpha = {0}; beta = {1}\".format(alpha, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same data we get values that are very close to the exact answers.\n",
    "\n",
    "So why choose least squares?  One justification involves *maximum likelihood estimation*.  Let's say we have a sample of data that comes from a distribution that depends on some unknown parameter:\n",
    "\n",
    "$$ p(v_1,...,v_n \\mid \\theta) $$\n",
    "\n",
    "If we didn't know theta, we could turn around and think of this quantity as the likelihood of $\\theta$ given the sample:\n",
    "\n",
    "$$ L(\\theta  \\mid v_1,...,v_n) $$\n",
    "\n",
    "Under this approach, the most likely $\\theta$ value is the value that maximizes this likelihood function; that is, the value that makes the observed data the most probable.  In the case of a continuous distributionm in which we have a probability distribution function rather than a probability mass function, we can do the same thing.\n",
    "\n",
    "One assumtion that's made about the simple regression model is that the regression errors are normally distributed with mean zero and some standard deviation of $\\sigma$.  If that's the case, then the likelihood based on seeing a pair `(x_i, y_i)` is:\n",
    "\n",
    "$$ L(\\alpha, \\beta \\mid x_p y_p \\sigma) = \\frac {1}{\\sqrt 2 \\pi \\sigma} \\exp ( - (y_i - \\alpha - \\beta x_i ) ^2 /2 \\sigma ^2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
