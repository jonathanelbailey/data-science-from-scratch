{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will begin where the last chapter left off, except now we are adding new data into the model.  We will now propose a linear model with more independent variables:\n",
    "\n",
    "$$ minutes = \\alpha + \\beta_1 friends + \\beta_2 work hours + \\beta_3 phd + \\epsilon $$\n",
    "\n",
    "Note that whether a user has a PhD is not a number; it is merely a *dummy variable* that equals 1 for users with PhDs and 0 for users without.\n",
    "\n",
    "Recall the following model:\n",
    "\n",
    "$$ y_1 = \\alpha + \\beta x_i + \\epsilon_i $$\n",
    "\n",
    "Imagine that each input $ x_i $ is not a single number but rather a vector of $ k $ numbers $ x_i1, ... , x_ik $.  The multiple regression model assumes:\n",
    "\n",
    "$$ y_i = \\alpha + \\beta_1x_i1 + ... + \\beta_k x_ik + \\epsilon_i $$\n",
    "\n",
    "In multiple regressions the vector of parameters is usually called $ \\beta $.  We'll want this to include a constant term as well, which we can achieve by adding a column of ones to our data:\n",
    "\n",
    "```python\n",
    "beta = [alpha, beta_1, ..., beta_k]\n",
    "x_i = [1, x_i1, ..., x_ik]\n",
    "```\n",
    "\n",
    "Then our model will be:\n",
    "\n",
    "```python\n",
    "def predict(x_i, beta):\n",
    "    return dot(x_i, beta)\n",
    "```\n",
    "\n",
    "Which in this case our independent variable `x` will be a list of vectors, each of which look like this:\n",
    "\n",
    "```python\n",
    "[1,    # constant term\n",
    " 49,   # number of friends\n",
    " 4,    # work hours per day\n",
    " 0]    # doesn't have a PhD\n",
    "```\n",
    "\n",
    "There are a couple of further assumptions that are required for this model to make sense:\n",
    "\n",
    "1. Columns of `x` are *linearly independent*, meaning that there's no way to write any one as a weighted sum of some of the others.  If this assumption fails, it's impossible to estimate `beta`.  For example, imagine we had an extra field `num_acquaintances` in our data that for every user was exactly the same as `num_friends`.\n",
    "2. Starting with any `beta`, if we add *any* amount to the `num_friends` coefficient and subtract that same amount from the `num_acquaintances` coefficient, the model's predictions will remain unchanged.  This means that there's no way to find the coefficient for `num_friends`.\n",
    "3. The columns of `x` are all uncorrelated with the errors $ \\epsilon $.  If this fails to be the case, our estimates of `beta` will be systematically wrong.  For example, we built a model that predicted that each additional friend was associated with an extra `0.90` daily minutes on the site.  Imagine that it's also the case that: people who work more hours spend less time on the site; and people with more friends tend to work more hours.  That is, imagine that the *actual* model is:\n",
    "\n",
    "$$ minutes = \\alpha + \\beta_1friends + \\beta_2work hours + \\epsilon $$\n",
    "\n",
    "and that work hours and friends are positively correlated.  In that case, when we minimize the errors of the single variable model\n",
    "\n",
    "$$ minutes = \\alpha + \\beta_1friends + \\epsilon $$\n",
    "\n",
    "we will underestimate $ \\beta_1 $.\n",
    "\n",
    "So what does this mean?\n",
    "\n",
    "If we made predictions using the single variable model with the \"actual\" value of `beta_1`, the predictions would tend to be too small for users who work many hours and too large for users who work few hours, because $ \\beta_2 > 0 $ and we \"forgot\" to include it.  Because work hours is positively correlated with number of friends, this means the predictions tend to be too small for users with many friends and too large for users with few friends.\n",
    "\n",
    "The result of this is that we can reduce the errors in the single variable model by decreasing our estimate of `beta_1`, which means that the error minimizing `beta_1` is smaller than the \"actual\" value.  So in this case, the single variable least squares solution is biased to underestimate `beta_1`.  And, in general, whenever the independent variables are correlated with the errors like this, our least squares solution will give us a biased estimate of $\\beta$.\n",
    "\n",
    "Just like in the simple linear model, we'll choose `beta` to minimize the sum of squared errors.  Finding an exact solution is not simple to do by hand, which means we'll need to use gradient descent.  We'll start by creating an error function to minimize.  For stochastic gradient descent, we'll just want the squared error corresponding to a single prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import partial\n",
    "from code_python3.linear_algebra import dot, vector_add\n",
    "from code_python3.stats import median, standard_deviation\n",
    "from code_python3.probability import normal_cdf\n",
    "from code_python3.gradient_descent import minimize_stochastic\n",
    "from code_python3.simple_linear_regression import total_sum_of_squares\n",
    "import math, random\n",
    "\n",
    "def predict(x_i, beta):\n",
    "    return dot(x_i, beta)\n",
    "\n",
    "def error(x_i, y_i, beta):\n",
    "    return y_i - predict(x_i, beta)\n",
    "\n",
    "def squared_error(x_i, y_i, beta):\n",
    "    return error(x_i, y_i, beta) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know calculus, you can compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error_gradient(x_i, y_i, beta):\n",
    "    \"\"\"the gradient corresponding to the ith squared error term\"\"\"\n",
    "    return [-2 * x_ij * error(x_i, y_i, beta)\n",
    "            for x_ij in x_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to find the optimal `beta` using stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minutes = 30.619881701311712 + 0.9702056472470465 friends -1.8671913880379478 work hours + 0.9163711597955347 PhD\n"
     ]
    }
   ],
   "source": [
    "def estimate_beta(x, y):\n",
    "    beta_initial = [random.random() for x_i in x[0]]\n",
    "    return minimize_stochastic(squared_error,\n",
    "                               squared_error_gradient,\n",
    "                               x, y,\n",
    "                               beta_initial,\n",
    "                               0.001)\n",
    "\n",
    "x = [[1,49,4,0],[1,41,9,0],[1,40,8,0],[1,25,6,0],[1,21,1,0],[1,21,0,0],[1,19,3,0],[1,19,0,0],[1,18,9,0],[1,18,8,0],[1,16,4,0],[1,15,3,0],[1,15,0,0],[1,15,2,0],[1,15,7,0],[1,14,0,0],[1,14,1,0],[1,13,1,0],[1,13,7,0],[1,13,4,0],[1,13,2,0],[1,12,5,0],[1,12,0,0],[1,11,9,0],[1,10,9,0],[1,10,1,0],[1,10,1,0],[1,10,7,0],[1,10,9,0],[1,10,1,0],[1,10,6,0],[1,10,6,0],[1,10,8,0],[1,10,10,0],[1,10,6,0],[1,10,0,0],[1,10,5,0],[1,10,3,0],[1,10,4,0],[1,9,9,0],[1,9,9,0],[1,9,0,0],[1,9,0,0],[1,9,6,0],[1,9,10,0],[1,9,8,0],[1,9,5,0],[1,9,2,0],[1,9,9,0],[1,9,10,0],[1,9,7,0],[1,9,2,0],[1,9,0,0],[1,9,4,0],[1,9,6,0],[1,9,4,0],[1,9,7,0],[1,8,3,0],[1,8,2,0],[1,8,4,0],[1,8,9,0],[1,8,2,0],[1,8,3,0],[1,8,5,0],[1,8,8,0],[1,8,0,0],[1,8,9,0],[1,8,10,0],[1,8,5,0],[1,8,5,0],[1,7,5,0],[1,7,5,0],[1,7,0,0],[1,7,2,0],[1,7,8,0],[1,7,10,0],[1,7,5,0],[1,7,3,0],[1,7,3,0],[1,7,6,0],[1,7,7,0],[1,7,7,0],[1,7,9,0],[1,7,3,0],[1,7,8,0],[1,6,4,0],[1,6,6,0],[1,6,4,0],[1,6,9,0],[1,6,0,0],[1,6,1,0],[1,6,4,0],[1,6,1,0],[1,6,0,0],[1,6,7,0],[1,6,0,0],[1,6,8,0],[1,6,4,0],[1,6,2,1],[1,6,1,1],[1,6,3,1],[1,6,6,1],[1,6,4,1],[1,6,4,1],[1,6,1,1],[1,6,3,1],[1,6,4,1],[1,5,1,1],[1,5,9,1],[1,5,4,1],[1,5,6,1],[1,5,4,1],[1,5,4,1],[1,5,10,1],[1,5,5,1],[1,5,2,1],[1,5,4,1],[1,5,4,1],[1,5,9,1],[1,5,3,1],[1,5,10,1],[1,5,2,1],[1,5,2,1],[1,5,9,1],[1,4,8,1],[1,4,6,1],[1,4,0,1],[1,4,10,1],[1,4,5,1],[1,4,10,1],[1,4,9,1],[1,4,1,1],[1,4,4,1],[1,4,4,1],[1,4,0,1],[1,4,3,1],[1,4,1,1],[1,4,3,1],[1,4,2,1],[1,4,4,1],[1,4,4,1],[1,4,8,1],[1,4,2,1],[1,4,4,1],[1,3,2,1],[1,3,6,1],[1,3,4,1],[1,3,7,1],[1,3,4,1],[1,3,1,1],[1,3,10,1],[1,3,3,1],[1,3,4,1],[1,3,7,1],[1,3,5,1],[1,3,6,1],[1,3,1,1],[1,3,6,1],[1,3,10,1],[1,3,2,1],[1,3,4,1],[1,3,2,1],[1,3,1,1],[1,3,5,1],[1,2,4,1],[1,2,2,1],[1,2,8,1],[1,2,3,1],[1,2,1,1],[1,2,9,1],[1,2,10,1],[1,2,9,1],[1,2,4,1],[1,2,5,1],[1,2,0,1],[1,2,9,1],[1,2,9,1],[1,2,0,1],[1,2,1,1],[1,2,1,1],[1,2,4,1],[1,1,0,1],[1,1,2,1],[1,1,2,1],[1,1,5,1],[1,1,3,1],[1,1,10,1],[1,1,6,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,4,1],[1,1,9,1],[1,1,9,1],[1,1,4,1],[1,1,2,1],[1,1,9,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,1,1],[1,1,1,1],[1,1,5,1]]\n",
    "daily_minutes_good = [68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,34.76,54.01,38.79,47.59,49.1,27.66,41.03,36.73,48.65,28.12,46.62,35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,36.32,20.45,21.93,26.02,27.34,23.49,46.94,30.5,33.8,24.23,21.4,27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,26.41,26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,36.31,21.03,30.86,36.07,28.66,29.08,37.28,15.28,24.17,22.31,30.17,25.53,19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,24.81,19.33,28.77,24.26,31.98,25.73,24.86,16.28,34.51,15.23,39.72,40.8,26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,35.59,39.43,14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,20.9,21.9,18.79,27.61,27.21,26.61,29.77,20.59,27.53,13.82,33.2,25,33.1,36.65,18.63,14.87,22.2,36.81,25.53,24.62,26.25,18.21,28.08,19.42,29.79,32.8,35.99,28.32,27.79,35.88,29.06,36.28,14.1,36.63,37.49,26.9,18.58,38.48,24.48,18.95,33.55,14.24,29.04,32.51,25.63,22.22,19,32.73,15.16,13.9,27.2,32.01,29.27,33,13.74,20.42,27.32,18.23,35.35,28.48,9.08,24.62,20.12,35.26,19.92,31.02,16.49,12.16,30.7,31.22,34.65,13.13,27.51,33.2,31.57,14.1,33.42,17.44,10.12,24.42,9.82,23.39,30.93,15.03,21.67,31.09,33.29,22.61,26.89,23.48,8.38,27.81,32.35,23.84]\n",
    "\n",
    "random.seed(0)\n",
    "beta = estimate_beta(x, daily_minutes_good)\n",
    "minutes = \"minutes = {0}\".format(beta[0])\n",
    "friends = \"+ {0} friends\".format(beta[1])\n",
    "work_hours = \"{0} work hours\".format(beta[2])\n",
    "phd = \"+ {0} PhD\".format(beta[3])\n",
    "print(minutes, friends, work_hours, phd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we interpret the model?  Let's think of the coefficients of the model as representing all-else-being-equal estimates of the impacts of each factor.  All else being equal, each additinoal friend corresponds to an extra minute spent on the site each day.  All else being equal, each additional hour in a user's workday corresponds to about two fewer minutes spent on the site each day.  All else being equal, having a PhD is associated with spending an extra minute on the site each day.\n",
    "\n",
    "What this doesn't tell us is anything about the interactions among the variables.  It's possible that the effect of work hours is different for people with many friends than it is for people with few friends.  This model doesn't capture that.  One way to handle this case is to introduce a new variable that is the *product* of \"friends\" and \"work hours\".  This effectively allows the \"work hours\" coefficient to increase or decrease as the number of friends fluctuates.\n",
    "\n",
    "Additionally, there may be a limit to how much time is spent depending on the coefficients.  We can account for this by adding yet another variable that is the *square* of the number of friends.  Once we start adding new variables, we'll need to begin worrying about whether or not they \"matter\".  There are no limits to the numbers of products, logs, squares, and higher powers we could add.\n",
    "\n",
    "If we look at the R-Squared, we'll see it has now increased to 0.68:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6800074955952597"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiple_r_squared(x, y, beta):\n",
    "    sum_of_squared_errors = sum(error(x_i, y_i, beta) ** 2\n",
    "                                for x_i, y_i in zip(x, y))\n",
    "    return 1.0 - sum_of_squared_errors / total_sum_of_squares(y)\n",
    "\n",
    "multiple_r_squared(x, daily_minutes_good, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that adding new variables to a regression will *necessarily* increase the R-Squared.  The simple regression model is just the special case of the multiple regression model where the coefficients on \"work hours\" and \"PhD\" both equal 0.  The optimal multiple regression model will necessarily have an error at least as small as that one.\n",
    "\n",
    "Because of this, in a multiple regression, we also need to look at the *standard errors of the coefficients*, which measure how certain we are about our estimates of each $ \\beta_i $.  The regression as a whole may fit our data very well, but if some of the independent variables are correlated (or irrelevant), their coefficients might not mean much.\n",
    "\n",
    "The typical approach to measuring these errors starts with another assumption.  The errors $ \\epsilon_i $ are independent normal random variables with mean 0 and some shared unknown standard deviation $ \\sigma $.  In that case, we can use some linear algebra to find the standard error of each coefficient.  Unfortunately, we're not set up to do that kind of linear algebra from scratch.\n",
    "\n",
    "So, imagine that we have a sample `n` data points, generated by some distribution:\n",
    "\n",
    "```python\n",
    "data = get_sample(num_points=n)\n",
    "```\n",
    "\n",
    "We've already written a function to compute the median of the observed data, which we can use as an estimate of the median of the distribution itself.  But how confident can we be about our estimate?  If all the data in the sample are very close to 100, then it seems likely that the actual median is close to 100.  If approximately half the data in the sample is close to 0 and the other half is close to 200, then we can't be nearly as certain about the median.  If we could repeatedly get new samples, we could compute the median of each and look at the distribution of those medians.  Usually we can't.  What we can do instead is *bootstrap* new data sets by choosing `n` data points with replacement from our data and then compute the mediansn of those synthetic data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap_statistic(close_to_100, median, 100):\n",
      "[100.02634537429101, 100.06586584030461, 100.06850285070729, 100.06586584030461, 100.08022923777746, 100.02634537429101, 100.0251392489555, 100.06586584030461, 100.0251392489555, 100.18315165595742, 100.06627711829937, 100.06586584030461, 100.06627711829937, 100.06627711829937, 100.08022923777746, 100.08022923777746, 99.97928565593025, 99.95763666335068, 100.06627711829937, 99.95763666335068, 100.06586584030461, 100.06627711829937, 100.06850285070729, 100.08732646013036, 100.0340458696825, 100.02634537429101, 100.16675086620182, 100.11133261277095, 100.02634537429101, 100.0340458696825, 100.02048509159759, 100.06586584030461, 100.0340458696825, 100.02048509159759, 100.06850285070729, 100.06850285070729, 99.95763666335068, 100.08022923777746, 100.02048509159759, 100.07674830805912, 100.06586584030461, 100.06850285070729, 100.06627711829937, 100.11133261277095, 100.08022923777746, 100.06627711829937, 100.02634537429101, 100.0251392489555, 100.06850285070729, 100.06627711829937, 100.06586584030461, 100.0251392489555, 100.08022923777746, 100.11133261277095, 100.06586584030461, 100.0251392489555, 100.02634537429101, 100.04153611616675, 100.08022923777746, 100.06627711829937, 100.19440587880706, 100.04153611616675, 99.98693719710383, 100.06850285070729, 100.07674830805912, 100.08022923777746, 100.08732646013036, 100.06627711829937, 100.04153611616675, 100.08732646013036, 100.11133261277095, 100.06586584030461, 100.0251392489555, 100.13015471695907, 100.02634537429101, 99.95728292174014, 100.08732646013036, 100.06850285070729, 99.92424918869057, 100.02048509159759, 99.97928565593025, 99.92424918869057, 100.13015471695907, 100.06586584030461, 100.08022923777746, 100.06850285070729, 100.04153611616675, 100.06627711829937, 100.06586584030461, 100.06850285070729, 100.02048509159759, 100.02634537429101, 100.06627711829937, 100.04153611616675, 100.02634537429101, 100.02634537429101, 100.06627711829937, 100.18315165595742, 100.06850285070729, 100.06627711829937]\n",
      "bootstrap_statistic(far_from_100, median, 100):\n",
      "[0.9746929609488131, 0.9746929609488131, 0.9632764195933914, 200.0254523027627, 200.03723940374888, 0.9740127081653535, 200.0254523027627, 200.03723940374888, 0.9740127081653535, 200.043346407288, 200.06556682467556, 0.9679946120377706, 0.9229610720300252, 0.9170498145371947, 0.9746929609488131, 200.02972914757345, 100.37472916630605, 200.0439362129153, 0.9170498145371947, 0.9746929609488131, 0.9819299666825538, 0.9740127081653535, 200.02972914757345, 0.9819299666825538, 200.0439362129153, 200.03611456820246, 200.0439362129153, 200.02972914757345, 200.03611456820246, 200.0254523027627, 200.03723940374888, 0.9819299666825538, 0.9229610720300252, 0.9413891648644847, 0.9413891648644847, 200.01173496311017, 0.9746929609488131, 200.02972914757345, 200.043346407288, 0.9679946120377706, 200.07626904987512, 200.01173496311017, 0.9819299666825538, 200.01173496311017, 0.9632764195933914, 0.9746929609488131, 0.9746929609488131, 200.01173496311017, 0.9632764195933914, 0.9746929609488131, 200.02972914757345, 0.8082392572934448, 0.9413891648644847, 200.11842771777464, 0.9229610720300252, 0.9229610720300252, 100.37472916630605, 0.9819299666825538, 0.9740127081653535, 200.01173496311017, 200.03611456820246, 0.9819299666825538, 200.01173496311017, 0.9632764195933914, 200.0254523027627, 200.02972914757345, 200.02972914757345, 200.03611456820246, 0.9413891648644847, 100.37472916630605, 0.9170498145371947, 200.03723940374888, 0.9632764195933914, 0.8082392572934448, 0.9170498145371947, 200.01173496311017, 0.9746929609488131, 0.9746929609488131, 200.0439362129153, 100.37472916630605, 200.01173496311017, 200.03611456820246, 200.0254523027627, 0.9632764195933914, 200.02972914757345, 0.9740127081653535, 200.0254523027627, 100.37472916630605, 200.07626904987512, 0.9679946120377706, 200.02972914757345, 200.03723940374888, 200.01173496311017, 200.07626904987512, 0.9746929609488131, 0.9740127081653535, 0.9819299666825538, 200.01173496311017, 100.37472916630605, 100.37472916630605]\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_sample(data):\n",
    "    \"\"\"randomly samples len(data) elements with replacement\"\"\"\n",
    "    return [random.choice(data) for _ in data]\n",
    "\n",
    "def bootstrap_statistic(data, stats_fn, num_samples):\n",
    "    \"\"\"evaluates stats_fn on num_samples bootstrap samples from data\"\"\"\n",
    "    return [stats_fn(bootstrap_sample(data))\n",
    "            for _ in range(num_samples)]\n",
    "\n",
    "# 101 points all very close to 100\n",
    "close_to_100 = [99.5 + random.random() for _ in range(101)]\n",
    "\n",
    "# 101 points, 50 of them near 0, 50 of them near 200\n",
    "far_from_100 = ([99.5 + random.random()] +\n",
    "                [random.random() for _ in range(50)] +\n",
    "                [200 + random.random() for _ in range(50)])\n",
    "\n",
    "print(\"bootstrap_statistic(close_to_100, median, 100):\")\n",
    "print(bootstrap_statistic(close_to_100, median, 100))\n",
    "print(\"bootstrap_statistic(far_from_100, median, 100):\")\n",
    "print(bootstrap_statistic(far_from_100, median, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `standard_deviation` of the first set of medians is close to 0, while the `standard_deviation` of the second set of medians is close to 100.  We can take the same approach to estimating the standard errors of our regression coefficients.  We repeatedly take a `bootstrap_sample` of our data and estimate `beta` based on that sample.  If the coefficient corresponding to one of the independent variables, say `num_friends`, doesn't vary much across samples, then we can be confident that our estimate is relatively tight.  If the coefficient varies greatly across samples, then we can't be at all confident in our estimate.\n",
    "\n",
    "The only subtlety is that before sampling, we'll need to `zip` our `x` data and `y` data to make sure that corresponding values of the independent and dependent variables are sampled together.  This means that `bootstrap_sample` will return a list of pairs(`x_i, y_i`), which we'll need to reassemble into an `x_sample` and a `y_sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.953551702104508, 0.06288763616183773, 0.11722269488203318, 0.8591786495949066]\n",
      "constant term: 0.953551702104508\n",
      "number of friends: 0.06288763616183773\n",
      "unemployed: 0.11722269488203318\n",
      "phd: 0.8591786495949066\n"
     ]
    }
   ],
   "source": [
    "def estimate_sample_beta(sample):\n",
    "    x_sample, y_sample = list(zip(*sample)) # magic unzipping trick\n",
    "    return estimate_beta(x_sample, y_sample)\n",
    "\n",
    "random.seed(0) # so that you get the same results as me\n",
    "\n",
    "bootstrap_betas = bootstrap_statistic(list(zip(x, daily_minutes_good)),\n",
    "                                      estimate_sample_beta,\n",
    "                                      100)\n",
    "\n",
    "bootstrap_standard_errors = [\n",
    "    standard_deviation([beta[i] for beta in bootstrap_betas])\n",
    "    for i in range(4)]\n",
    "\n",
    "print(bootstrap_standard_errors)\n",
    "constant_term = \"constant term: {}\".format(bootstrap_standard_errors[0])\n",
    "num_friends = \"number of friends: {}\".format(bootstrap_standard_errors[1])\n",
    "unemployed = \"unemployed: {}\".format(bootstrap_standard_errors[2])\n",
    "phd = \"phd: {}\".format(bootstrap_standard_errors[3])\n",
    "print(constant_term)\n",
    "print(num_friends)\n",
    "print(unemployed)\n",
    "print(phd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these to test hypotheses such as \"does $ \\beta_i $ equal zero?\"  Under the null hypothesis $ \\beta_i = 0 $ and with our other assumptions about the distribution of $ \\epsilon_i $ the statistic:\n",
    "$$ t_j = \\beta_j/\\sigma_j $$\n",
    "\n",
    "which is our estimate of $ \\beta_j $ divided by our estimate of its standard error, follows a t-distribution with \"`n-k` degrees of freedom.\"  We could create a `students_t_cdf` function to compute p-values for each least-squares coefficient to indicate how likely we would be to observe such a value if the actual coefficient were zero.  But that's outside of scope.  As degrees of freedom get large, the t-distribution gets closer and closer to a standard normal.  In a situation like this, where `n` is much larger than `k`, we can use `normal_cdf` and still feel good about ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value(30.63, 1.174) 0.0\n",
      "p_value(0.972, 0.079) 0.0\n",
      "p_value(-1.868, 0.131) 0.0\n",
      "p_value(0.911, 0.990) 0.35746719881669264\n"
     ]
    }
   ],
   "source": [
    "def p_value(beta_hat_j, sigma_hat_j):\n",
    "    if beta_hat_j > 0:\n",
    "        return 2 * (1 - normal_cdf(beta_hat_j / sigma_hat_j))\n",
    "    else:\n",
    "        return 2 * normal_cdf(beta_hat_j / sigma_hat_j)\n",
    "\n",
    "print(\"p_value(30.63, 1.174)\", p_value(30.63, 1.174))\n",
    "print(\"p_value(0.972, 0.079)\", p_value(0.972, 0.079))\n",
    "print(\"p_value(-1.868, 0.131)\", p_value(-1.868, 0.131))\n",
    "print(\"p_value(0.911, 0.990)\", p_value(0.911, 0.990))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While most of the coefficients have very small p-values, the coefficient for \"PhD\" is not \"significantly\" different from zero, which makes it likely that the coefficient for \"PhD\" is random rather than meaningful.  In more elaborate regression scenarios, you sometimes want to test more elaborate hypotheses about the data, which you can do with an F-Test, which is also outside scope.\n",
    "\n",
    "In practice, you'd often like to apply linear regression to data sets with large numbers of variables.  This creates a couple of extra issues.  First, the more variables you use, the more likely you are to overfit your model to the training set.  And second, the more nonzero coefficients you have, the harder it is to make sense of them if the goal is to explain some phenomenon, a sparse model with three factors might be more useful than a slightly better model with hundreds.\n",
    "\n",
    "*Regularization* is an approach in which we add to the error term a penalty that gets larger as `beta` gets larger.  We then minimize the combined error and penalty.  The more importance we place on the penalty term, the more we discourage large coefficients.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha is a *hyperparameter* controlling how harsh the penalty is\n",
    "# sometimes it's called \"lambda\" but that already means something in Python\n",
    "def ridge_penalty(beta, alpha):\n",
    "  return alpha * dot(beta[1:], beta[1:])\n",
    "\n",
    "def squared_error_ridge(x_i, y_i, beta, alpha):\n",
    "    \"\"\"estimate error plus ridge penalty on beta\"\"\"\n",
    "    return error(x_i, y_i, beta) ** 2 + ridge_penalty(beta, alpha)\n",
    "\n",
    "def ridge_penalty_gradient(beta, alpha):\n",
    "    \"\"\"gradient of just the ridge penalty\"\"\"\n",
    "    return [0] + [2 * alpha * beta_j for beta_j in beta[1:]]\n",
    "\n",
    "def squared_error_ridge_gradient(x_i, y_i, beta, alpha):\n",
    "    \"\"\"the gradient corresponding to the ith squared error term\n",
    "    including the ridge penalty\"\"\"\n",
    "    return vector_add(squared_error_gradient(x_i, y_i, beta),\n",
    "                      ridge_penalty_gradient(beta, alpha))\n",
    "\n",
    "def estimate_beta_ridge(x, y, alpha):\n",
    "    \"\"\"use gradient descent to fit a ridge regression\n",
    "    with penalty alpha\"\"\"\n",
    "    beta_initial = [random.random() for x_i in x[0]]\n",
    "    return minimize_stochastic(partial(squared_error_ridge, alpha=alpha),\n",
    "                               partial(squared_error_ridge_gradient,\n",
    "                                       alpha=alpha),\n",
    "                               x, y,\n",
    "                               beta_initial,\n",
    "                               0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `alpha` set to zero, there's no penalty at all and we get the same results as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0 = [30.619881701311712, 0.9702056472470465, -1.8671913880379478, 0.9163711597955347]\n",
      "dot = 5.267438780018153\n",
      "multiple_r_squared = 0.6800074955952597\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "beta_0 = estimate_beta_ridge(x, daily_minutes_good, alpha=0.0)\n",
    "print(\"beta_0 = {0}\".format(beta_0))\n",
    "print(\"dot = {0}\".format(dot(beta_0[1:], beta_0[1:])))\n",
    "print(\"multiple_r_squared = {0}\".format(multiple_r_squared(x, daily_minutes_good, beta_0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase `alpha`, the goodness of fit gets worse, but the size of `beta` gets smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0.0\n",
      "beta [30.640219204857612, 0.967957481356752, -1.868740132988584, 0.9147860085860332]\n",
      "dot(beta[1:],beta[1:]) 5.265964811861464\n",
      "r-squared 0.6800017437926997\n",
      "\n",
      "alpha 0.01\n",
      "beta [30.624475435920044, 0.9671382470586483, -1.8600056698394003, 0.8908465529845938]\n",
      "dot(beta[1:],beta[1:]) 5.188585061722923\n",
      "r-squared 0.6799988103788881\n",
      "\n",
      "alpha 0.1\n",
      "beta [30.90734173857905, 0.9447829718748573, -1.8488162568510678, 0.5025503230840757]\n",
      "dot(beta[1:],beta[1:]) 4.56329324277339\n",
      "r-squared 0.6796752768537219\n",
      "\n",
      "alpha 1\n",
      "beta [30.67561351146747, 0.9059985240061532, -1.692308153612175, 0.08524712362724758]\n",
      "dot(beta[1:],beta[1:]) 3.692007284370296\n",
      "r-squared 0.6755890872793071\n",
      "\n",
      "alpha 10\n",
      "beta [28.341832707881014, 0.7303604899789848, -0.9137372493170081, -0.01795074775901734]\n",
      "dot(beta[1:],beta[1:]) 1.368664435456863\n",
      "r-squared 0.5737416601918544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.0, 0.01, 0.1, 1, 10]:\n",
    "    beta = estimate_beta_ridge(x, daily_minutes_good, alpha=alpha)\n",
    "    print(\"alpha\", alpha)\n",
    "    print(\"beta\", beta)\n",
    "    print(\"dot(beta[1:],beta[1:])\", dot(beta[1:], beta[1:]))\n",
    "    print(\"r-squared\", multiple_r_squared(x, daily_minutes_good, beta))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is lasso regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_penalty(beta, alpha):\n",
    "    return alpha * sum(abs(beta_i) for beta_i in beta[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the ridge penalty shrank the coefficients overall, the lasso penalty tends to force coefficients to be zero, which makes it good for learning sparse models.  Unfortunately, it's not amenable to gradient descent, which means that we won't be able to solve it from scratch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
