{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baye's theorem allows for introduction of prior assumptions into the probability formula for a current set of events:\n",
    "\n",
    "$$ P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)} $$\n",
    "\n",
    "Read as a word problem, it's quite confusing:\n",
    "> the probability of `A` given `B` is equal to the probability of `B` given `A` times the probability of `A` divided by the probability of `B`\n",
    "\n",
    "In order to explain Bayes' theorem, let's create an example of its usage.  Imagine that we are consistently receiving messages at random.  Let `S` be the event 'the message is spam', and `V` be the event 'the message contains the word viagra'.  Then, Bayes' theorem tells us that the probability that the message is spam conditional on containing the word viagra is:\n",
    "\n",
    "$$ P(S \\mid V) = \\frac{P(V \\mid S) \\, P(S)}{P(V \\mid S) \\, P(S) + P(V \\mid -S) \\, P(-S)} $$\n",
    "\n",
    "The numerator is the probability that a message is spam *and* contains *viagra*.  The denominator is just the probability that a message contains *viagra*.  Hence, you can think of this calculation as simply representing the proportion of *viagra* messages that are spam.\n",
    "\n",
    "If we have a data set with data properly labeled as either spam or not spam, then we can estimate `P(V|S)` and `P(V|-S)`, respectively.  We can further assume that any message is equally likely to be spam or not spam so that `P(S) = P(-S) = 0.5`, then:\n",
    "\n",
    "$$ P(S \\mid V) = \\frac{P(V \\mid S)}{P(V \\mid S) + P(V \\mid -S) \\, P(-S)} $$\n",
    "\n",
    "For example, if 50% of spam messages have the word *viagra*, but only 1% of nonspam messages do, then the probability that any given *viagra*-containing emails is spam is:\n",
    "\n",
    "$$ \\frac{0.5}{(0.5 + 0.01)} = 98% $$\n",
    "\n",
    "Which is quite acceptable.  Next, let's expand our vocabulary to many words `w1...wn`.  To move this into the realm of probability theory, we'll write `Xi` for the event 'a message contains the word `wi`'.  Our estimate would look like:\n",
    "\n",
    "$$ P(X_i \\mid S) $$\n",
    "\n",
    "This represents the probability that a spam message contains the `i`th word, and a similar estimate:\n",
    "\n",
    "$$ P(X_i \\mid -S) $$\n",
    "\n",
    "represents the probability that a nonspam message contains the `i`th word.  The key to naive Bayes is making the assumption that the presences or absences of each word are independent of one another, and they are conditional on a message being spam or not.  Intuitively, this assumption means that knowing whether a certain spam message contains the word 'viagra' gives you no information about whether the same message contains the word 'rolex'.  In math terms, this means that:\n",
    "\n",
    "$$ P(X_1 = x_1, ..., X_n = x_n \\mid S) = P(X_1 = x1 \\mid S) \\times ... \\times P(X_n = x_n \\mid S) $$\n",
    "\n",
    "This is an extreme assumption.  If half of all spam messages are for viagra and the other half are for cheap rolexes, this means that:\n",
    "\n",
    "$$ P(X_1 = 1, X_2 = 1 \\mid S) = P(X_1 = 1 \\mid S) P(X_2 = 1 \\mid S) = .5 \\times .5 = .25 $$\n",
    "\n",
    "Since we've assumed away the knowledge that viagra and rolex actually never occur together, we know that this assumption is a bit ridiculous.  Despite the unrealistic assumptions made by the model, it often performs well and is used in actual  spam filters.\n",
    "\n",
    "Next, we can use the single spam word formula to calculate:\n",
    "\n",
    "$$ P(S \\mid X = x) = \\frac{P(X = x \\mid S)}{[P(X = x \\mid S) + P(X = x \\mid -S)]} $$\n",
    "\n",
    "The naive Bayes assumption allows us to compute each of the probabilities on the right simply by multiplying together the individual probability estimate for each vocabulary word.  In practice, you usually want to avoid multiplying lots of probabilities together, to avoid a problem called *underflow*, in which computers don't deal well withs floating-point numbers that are too close to zero.  Recalling from algebra that:\n",
    "\n",
    "$$ \\log (ab) = \\log a + \\log b $$\n",
    "$$ \\exp (\\log x) = x $$\n",
    "\n",
    "we usually compute `p_1 *...* p_n` as the equivalent (but floating-point-friendlier):\n",
    "\n",
    "$$ \\exp (\\log (p_1) + ... + \\log (p_n)) $$\n",
    "\n",
    "The only challenge left is coming up with estimates for:\n",
    "\n",
    "$$ P(X_i \\mid S) $$\n",
    "$$ P(X_i \\mid -S) $$\n",
    "\n",
    "In order to get the probabilities that a spam message contains the word  `w_i`.  If we have a fair number of training messages labeled as spam and not-spam, an obvious first try is to estimate `P(X_i|S)` simply as the fraction of spam messages containing word `w_i`.\n",
    "\n",
    "This causes a big problem though.  Imagine that in our training set the vocabulary word 'data' only occurs in nonspam messages.  Then we'd estimate `P('data' | S) = 0`.  The result is that our Naive Bayes classifier would always assign spam probability `0` to *any* message containing the word 'data', even a message like 'data on cheap viagra and authentic rolex watches'.  To avoid this problem, we usually use some kind of smoothing.\n",
    "\n",
    "In particular, we'll choose a pseudocount `k` and estimate the probability of seeing the`i`th word in a spam as:\n",
    "\n",
    "$$ P(X_i \\mid S) = \\frac{(k + number\\ of\\ spams\\ containing\\ w_i)}{2k + number\\ of\\ spams} $$\n",
    "\n",
    "Similarly for `P(X_i | -S)`.  That is, when computing the spam probabilities for the `i`th word, we assume we also saw `k` additional spams containing the word and `k` additional spams not containing the word.\n",
    "\n",
    "Now, let's see what this looks like in python.  First, we need to build a function to tokenize messages into distinct words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from code_python3.machine_learning import split_data\n",
    "import math, random, re, glob\n",
    "\n",
    "def tokenize(message):\n",
    "    message = message.lower()                       # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   # extract the words\n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to count the words in a labeled training set of messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(training_set):\n",
    "    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for message, is_spam in training_set:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll turn the counts into estimated probabilities using the smoothing we described before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    \"\"\"turn the word_counts into a list of triplets\n",
    "    w, p(w | spam) and p(w | ~spam)\"\"\"\n",
    "    return [(w,\n",
    "             (spam + k) / (total_spams + 2 * k),\n",
    "             (non_spam + k) / (total_non_spams + 2 * k))\n",
    "             for w, (spam, non_spam) in counts.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last piece is to use these word probabilities to assign probabilities to messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_probability(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "\n",
    "        # for each word in the message,\n",
    "        # add the log probability of seeing it\n",
    "        if word in message_words:\n",
    "            log_prob_if_spam += math.log(prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "\n",
    "        # for each word that's not in the message\n",
    "        # add the log probability of _not_ seeing it\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "\n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put this all together into a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "\n",
    "    def train(self, training_set):\n",
    "\n",
    "        # count spam and non-spam messages\n",
    "        num_spams = len([is_spam\n",
    "                         for message, is_spam in training_set\n",
    "                         if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "\n",
    "        # run training data through our \"pipeline\"\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts,\n",
    "                                             num_spams,\n",
    "                                             num_non_spams,\n",
    "                                             self.k)\n",
    "\n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the data set from http://spamassassin.apache.org/old/publiccorpus/, let's get to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/home/jovyan/work/spam_data/*/*'\n",
    "\n",
    "def get_subject_data(path):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # regex for stripping out the leading \"Subject:\" and any spaces after it\n",
    "    subject_regex = re.compile(r\"^Subject:\\s+\")\n",
    "\n",
    "    # glob.glob returns every filename that matches the wildcarded path\n",
    "    for fn in glob.glob(path):\n",
    "        is_spam = \"ham\" not in fn\n",
    "\n",
    "        with open(fn,'r',encoding='ISO-8859-1') as file:\n",
    "            for line in file:\n",
    "                if line.startswith(\"Subject:\"):\n",
    "                    subject = subject_regex.sub(\"\", line).strip()\n",
    "                    data.append((subject, is_spam))\n",
    "                    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the data into training data and test data, and then we're ready to build a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(False, False): 716, (True, True): 85, (True, False): 49, (False, True): 26})\n",
      "spammiest_hams [('Species at risk of extinction growing', False, 0.8958889624800298), ('Cell phones coming soon', False, 0.9666801692557617), ('Adam dont job for no one, see.', False, 0.9758486261566025), ('2000+ year old Greek computer reinterpreted', False, 0.9767939458812925), ('Save up to 70% on international calls!', False, 0.9776715683050723)]\n",
      "hammiest_spams [('I was so scared... my very first DP', True, 5.406261641572762e-05), ('Re: Hi', True, 0.0009722322165778127), ('*****SPAM*****', True, 0.0021267760018624494), ('http://www.efi.ie/', True, 0.007758971914390464), ('Outstanding Opportunities for \"Premier Producers\"', True, 0.008355413144240641)]\n",
      "spammiest_words [('zzzz', 0.02837837837837838, 0.0002294630564479119), ('money', 0.033783783783783786, 0.0002294630564479119), ('rates', 0.033783783783783786, 0.0002294630564479119), ('systemworks', 0.033783783783783786, 0.0002294630564479119), ('adv', 0.0445945945945946, 0.0002294630564479119)]\n",
      "hammiest_words [('spambayes', 0.0013513513513513514, 0.04612207434603029), ('users', 0.0013513513513513514, 0.039238182652592934), ('was', 0.0013513513513513514, 0.03786140431390546), ('razor', 0.0013513513513513514, 0.034189995410738874), ('zzzzteana', 0.0013513513513513514, 0.030059660394676457)]\n"
     ]
    }
   ],
   "source": [
    "def p_spam_given_word(word_prob):\n",
    "    word, prob_if_spam, prob_if_not_spam = word_prob\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "\n",
    "def train_and_test_model(path):\n",
    "\n",
    "    data = get_subject_data(path)\n",
    "    random.seed(0)      # just so you get the same answers as me\n",
    "    train_data, test_data = split_data(data, 0.75)\n",
    "\n",
    "    classifier = NaiveBayesClassifier()\n",
    "    classifier.train(train_data)\n",
    "\n",
    "    classified = [(subject, is_spam, classifier.classify(subject))\n",
    "              for subject, is_spam in test_data]\n",
    "\n",
    "    counts = Counter((is_spam, spam_probability > 0.5) # (actual, predicted)\n",
    "                     for _, is_spam, spam_probability in classified)\n",
    "\n",
    "    print(counts)\n",
    "\n",
    "    classified.sort(key=lambda row: row[2])\n",
    "    spammiest_hams = list(filter(lambda row: not row[1], classified))[-5:]\n",
    "    hammiest_spams = list(filter(lambda row: row[1], classified))[:5]\n",
    "\n",
    "    print(\"spammiest_hams\", spammiest_hams)\n",
    "    print(\"hammiest_spams\", hammiest_spams)\n",
    "\n",
    "    words = sorted(classifier.word_probs, key=p_spam_given_word)\n",
    "\n",
    "    spammiest_words = words[-5:]\n",
    "    hammiest_words = words[:5]\n",
    "\n",
    "    print(\"spammiest_words\", spammiest_words)\n",
    "    print(\"hammiest_words\", hammiest_words)\n",
    "    \n",
    "train_and_test_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives 85 true positives, 49 false negatives, 26 false positives, and 716 true negatives, meaning our model is `85/(85+49) = 63%`, which seems pretty suboptimal.  But with work it should get better results.\n",
    "\n",
    "*note: the author's implementation got a precision result of 73%*\n",
    "\n",
    "There are several ways to improve accuracy:\n",
    "\n",
    "- expand the data set\n",
    "- look at the message content, not just the subject line\n",
    "- modify the classifier to accept an optional `min_count` threshold and ignore tokens that don't appear at least that many times\n",
    "- the tokenizer has no notion of similar words.  Modify the classifier to take an optional stemmer function that converts words to equivalence classes of words.  Possibly use PorterStemmer\n",
    "- More inputs could be added that search for domain name, or some other property that may be important"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
