{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce is a programming model for performing parallel processing on large data sets.  Although it is a powerful technique, its basics are relatively simple.  Imagine we have a collection of items we'd like to process somehow.  For instance, the items might be website logs, the texts of various books, image files, etc.  A basic version of the MapReduce algorithm consists of the following steps:\n",
    "\n",
    "1. Use a mapper function to turn each item into zero or more key-value pairs.\n",
    "2. Collect together all the pairs with identical keys.\n",
    "3. Use a reducer function on each collection of grouped values to produce output values for the corresponding keys.\n",
    "\n",
    "This is all sort of abstract, so let's look at a specific example.  There are few absolute rules of data science, but one of them is that your first MapReduce example has to involve counting words.\n",
    "\n",
    "when you have a small data set this is simple to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, re, datetime\n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial\n",
    "from code_python3.naive_bayes import tokenize\n",
    "\n",
    "def word_count_old(documents):\n",
    "    \"\"\"word count not using MapReduce\"\"\"\n",
    "    return Counter(word\n",
    "        for document in documents\n",
    "        for word in tokenize(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a massive data set, the set of documents is suddenly too big to fit on your computer.  If you can just fit this into the MapReduce model, you can use some \"big data\" infrastructure that your engineers have implemented.  First, we need a function that turns a document into a sequence of key-value pairs.  We'll want our output to be grouped by word, which means that the keys should be words.  And for each word, we'll just emit the value 1 to indicate that this pair corresponds to one occurence of the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_mapper(document):\n",
    "    \"\"\"for each word in the document, emit (word,1)\"\"\"\n",
    "    for word in tokenize(document):\n",
    "        yield (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping the plumbing step 2 for the moment, imagine that for some word we've collected a list of the corresponding counts we emitted.  Then to produce the overall count for that word we just need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_reducer(word, counts):\n",
    "    \"\"\"sum up the counts for a word\"\"\"\n",
    "    yield (word, sum(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to step 2, we now need to collect the results from `wc_mapper` and feed them to `wc_reducer`.  Let's think about how we would do this on just one computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(documents):\n",
    "    \"\"\"count the words in the input documents using MapReduce\"\"\"\n",
    "\n",
    "    # place to store grouped values\n",
    "    collector = defaultdict(list)\n",
    "\n",
    "    for document in documents:\n",
    "        for word, count in wc_mapper(document):\n",
    "            collector[word].append(count)\n",
    "\n",
    "    return [output\n",
    "            for word, counts in collector.items()\n",
    "            for output in wc_reducer(word, counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image that we have three documents `[\"data science\", \"big data\", \"science fiction\"]`.  Then `wc_mapper` applied to the first document yieldxs the two pairs `(\"data\", 1)` and `(\"science\", 1)`.  After we've gone through all three documents, the collector contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wc_mapper results\n",
      "[('data', 1), ('science', 1), ('big', 1), ('data', 1), ('science', 1), ('fiction', 1)]\n",
      "\n",
      "word count results\n",
      "[('data', 2), ('science', 2), ('big', 1), ('fiction', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents = [\"data science\", \"big data\", \"science fiction\"]\n",
    "\n",
    "wc_mapper_results = [result\n",
    "                     for document in documents\n",
    "                     for result in wc_mapper(document)]\n",
    "\n",
    "print(\"wc_mapper results\")\n",
    "print(wc_mapper_results)\n",
    "print()\n",
    "\n",
    "print(\"word count results\")\n",
    "print(word_count(documents))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the right infrastructure, we can do the following:\n",
    "\n",
    "- Have each machine run the mapper on its documents, producing lots of key-value pairs.\n",
    "- Distribute those key-value pairs to a number of \"reducing\" machines, making sure that the pairs corresponding to any given key all end up on the same machine.\n",
    "- Have each reducing machine group the pairs by key and then run the reducer on each set of values.\n",
    "- Return each key-output pair.\n",
    "\n",
    "What is amazing about this is that it scales horizontally.  If we double the number of machines, then our computation should run approximately twice as fast.  Each mapper machine will only need to do half as much work, and (assuming there are enough distinct keys to further distribute the reducer work) the same is true for the reducer machines.\n",
    "\n",
    "If you think about it for a minute, all of the word-count specific code in the previous example is contained in the `wc_mapper` and `wc_reducer` functions.  This means that witha  couple of changes, we have a much more general framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(documents):\n",
    "    \"\"\"count the words in the input documents using MapReduce\"\"\"\n",
    "\n",
    "    # place to store grouped values\n",
    "    collector = defaultdict(list)\n",
    "\n",
    "    for document in documents:\n",
    "        for word, count in wc_mapper(document):\n",
    "            collector[word].append(count)\n",
    "\n",
    "    return [output\n",
    "            for word, counts in collector.items()\n",
    "            for output in wc_reducer(word, counts)]\n",
    "\n",
    "def map_reduce(inputs, mapper, reducer):\n",
    "    \"\"\"runs MapReduce on the inputs using mapper and reducer\"\"\"\n",
    "    collector = defaultdict(list)\n",
    "\n",
    "    for input in inputs:\n",
    "        for key, value in mapper(input):\n",
    "            collector[key].append(value)\n",
    "\n",
    "    return [output\n",
    "            for key, values in collector.items()\n",
    "            for output in reducer(key,values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can count words simply by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count using map_reduce function\n",
      "[('data', 2), ('science', 2), ('big', 1), ('fiction', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"word count using map_reduce function\")\n",
    "print(map_reduce(documents, wc_mapper, wc_reducer))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the flexibility to solve a wide variety of problems.  Before we proceed, observe that `wc_reducer` is just summing the values corresponding to each key.  This kind of aggregation is common enough that it's worth abstracting it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_with(aggregation_fn, key, values):\n",
    "    \"\"\"reduces a key-values pair by applying aggregation_fn to the values\"\"\"\n",
    "    yield (key, aggregation_fn(values))\n",
    "\n",
    "def values_reducer(aggregation_fn):\n",
    "    \"\"\"turns a function (values -> output) into a reducer\"\"\"\n",
    "    return partial(reduce_with, aggregation_fn)\n",
    "\n",
    "sum_reducer = values_reducer(sum)\n",
    "max_reducer = values_reducer(max)\n",
    "min_reducer = values_reducer(min)\n",
    "count_distinct_reducer = values_reducer(lambda values: len(set(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a way to analyze status updates.  Given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_updates = [\n",
    "    {\"id\": 1,\n",
    "     \"username\" : \"joelgrus\",\n",
    "     \"text\" : \"Is anyone interested in a data science book?\",\n",
    "     \"created_at\" : datetime.datetime(2013, 12, 21, 11, 47, 0),\n",
    "     \"liked_by\" : [\"data_guy\", \"data_gal\", \"bill\"] },\n",
    "    # add your own\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we need to figure out which day of the week people talk the most about data science.  In order to find this, we'll just count how many data science updates there are on each day of the week.  This means we'll need to group by the day of week, so that's our key.  And if we emit a value of 1 for each update that contains \"data science\", we can simply get the total number using `sum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data science days\n",
      "[(5, 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def data_science_day_mapper(status_update):\n",
    "    \"\"\"yields (day_of_week, 1) if status_update contains \"data science\" \"\"\"\n",
    "    if \"data science\" in status_update[\"text\"].lower():\n",
    "        day_of_week = status_update[\"created_at\"].weekday()\n",
    "        yield (day_of_week, 1)\n",
    "\n",
    "data_science_days = map_reduce(status_updates,\n",
    "                               data_science_day_mapper,\n",
    "                               sum_reducer)\n",
    "\n",
    "print(\"data science days\")\n",
    "print(data_science_days)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a slightly more complicated example, imagine we need to find out for each user the most common word that she puts into her status updates.  There are three possible approaches that spring to mind for the `mapper`:\n",
    "\n",
    "- Put the username in the key; put the words and counts in the values.\n",
    "- Put the word in key; put the usernames and counts in the values.\n",
    "- Put the username and word in the key; put the counts in the values.\n",
    "\n",
    "If you think about it a bit more, we definitely want to group by `username` because we want to consider each person's words separately.  And we don't want to group by `word` since our reducer will need to see all the words for each person to find out which is the most popular.  This means that the first option is the right choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user words\n",
      "[('joelgrus', ('in', 1))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def words_per_user_mapper(status_update):\n",
    "    user = status_update[\"username\"]\n",
    "    for word in tokenize(status_update[\"text\"]):\n",
    "        yield (user, (word, 1))\n",
    "\n",
    "def most_popular_word_reducer(user, words_and_counts):\n",
    "    \"\"\"given a sequence of (word, count) pairs,\n",
    "    return the word with the highest total count\"\"\"\n",
    "\n",
    "    word_counts = Counter()\n",
    "    for word, count in words_and_counts:\n",
    "        word_counts[word] += count\n",
    "\n",
    "    word, count = word_counts.most_common(1)[0]\n",
    "\n",
    "    yield (user, (word, count))\n",
    "\n",
    "user_words = map_reduce(status_updates,\n",
    "                        words_per_user_mapper,\n",
    "                        most_popular_word_reducer)\n",
    "\n",
    "print(\"user words\")\n",
    "print(user_words)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could find out the number of distinct status-likers for each user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct likers\n",
      "[('joelgrus', 3)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def liker_mapper(status_update):\n",
    "    user = status_update[\"username\"]\n",
    "    for liker in status_update[\"liked_by\"]:\n",
    "        yield (user, liker)\n",
    "\n",
    "distinct_likers_per_user = map_reduce(status_updates,\n",
    "                                      liker_mapper,\n",
    "                                      count_distinct_reducer)\n",
    "\n",
    "print(\"distinct likers\")\n",
    "print(distinct_likers_per_user)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a $ m x n $ matrix *A* and a $ n x k $ matrix *B*, we can multiply them  to form a $ m x k $ matrix *C*, where the element of C in row *i* and column *j* is given by:\n",
    "\n",
    "$$ C_ij = A_i1B_1j + A_i2B_2j + ... + A_inB_nj $$\n",
    "\n",
    "A natural way to reprent this kind of matrix is with a list of lists.  But large matrices are sometimes sparse, which means that most of their elements equal zero.  For large sparse matrices, a list of lists can be a very wasteful representation.  A more compact representation is a list of tuples `(name, i, j, value)` where `name` identifies the matrix, and where `i, j, value` indicates a location with nonzero value.\n",
    "\n",
    "For example, a billion x billion matrix has a quintillion entries, which would not be easy to store on a computer.  But if there are only a few nonzero entries in each row, this alternativen representation is many orders of magnitude smaller.  Given this sort of representation, it turns out that we can use MapReduce to perform matrix multiplication in a distributed manner.\n",
    "\n",
    "To motivate our algorithm, notice that each element is only used to compute the elements of the matrix iby row, and each second tuple element is only used to compute the elements of the matrix by column.  Our goal will be for each output of our reducer to be a single entry of the matrix, which means that we'll need our mapper to emit keys identifying a single entry of the matrix.  This suggests the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_multiply_mapper(m, element):\n",
    "    \"\"\"m is the common dimension (columns of A, rows of B)\n",
    "    element is a tuple (matrix_name, i, j, value)\"\"\"\n",
    "    matrix, i, j, value = element\n",
    "\n",
    "    if matrix == \"A\":\n",
    "        for column in range(m):\n",
    "            # A_ij is the jth entry in the sum for each C_i_column\n",
    "            yield((i, column), (j, value))\n",
    "    else:\n",
    "        for row in range(m):\n",
    "            # B_ij is the ith entry in the sum for each C_row_j\n",
    "            yield((row, j), (i, value))\n",
    "\n",
    "def matrix_multiply_reducer(m, key, indexed_values):\n",
    "    results_by_index = defaultdict(list)\n",
    "    for index, value in indexed_values:\n",
    "        results_by_index[index].append(value)\n",
    "\n",
    "    # sum up all the products of the positions with two results\n",
    "    sum_product = sum(results[0] * results[1]\n",
    "                      for results in results_by_index.values()\n",
    "                      if len(results) == 2)\n",
    "\n",
    "    if sum_product != 0.0:\n",
    "        yield (key, sum_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if you had the two matrices\n",
    "\n",
    "```python\n",
    "A = [[3, 2, 0],\n",
    "     [0, 0, 0]]\n",
    "\n",
    "B = [[4, -1, 0],\n",
    "     [10, 0, 0],\n",
    "     [0, 0, 0]]\n",
    "```\n",
    "\n",
    "You could rewrite them as tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map-reduce matrix multiplication\n",
      "entries: [('A', 0, 0, 3), ('A', 0, 1, 2), ('B', 0, 0, 4), ('B', 0, 1, -1), ('B', 1, 0, 10)]\n",
      "result: [((0, 0), 32), ((0, 1), -3)]\n"
     ]
    }
   ],
   "source": [
    "entries = [(\"A\", 0, 0, 3), (\"A\", 0, 1,  2),\n",
    "       (\"B\", 0, 0, 4), (\"B\", 0, 1, -1), (\"B\", 1, 0, 10)]\n",
    "mapper = partial(matrix_multiply_mapper, 3)\n",
    "reducer = partial(matrix_multiply_reducer, 3)\n",
    "\n",
    "print(\"map-reduce matrix multiplication\")\n",
    "print(\"entries:\", entries)\n",
    "print(\"result:\", map_reduce(entries, mapper, reducer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
