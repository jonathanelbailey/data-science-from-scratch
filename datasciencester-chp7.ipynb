{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we test hypotheses.  Firstly, let's test the hypothesis that a series of coin flips will be fair.  It also build upon previous functions found in earlier chapters.\n",
    "\n",
    "### Assumptions:\n",
    "\n",
    "1. each flip is a Bernoulli trial, meaning that `X` a binomial `(n,p)` random variable.\n",
    "2. `X` can be approximated using normal distribution.\n",
    "3. Normal CDF is the probability that a var is below a threshold.\n",
    "4. anything not below the threshold is considered to be above the threshold.\n",
    "5. A var that's less than `hi` but not less than `lo` is considered to be between threshold.\n",
    "6. A var that is not between is considered outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Bernoulli trial #1\n",
    "def normal_approximation_to_binomial(n, p):\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(p * (1 - p) * n)\n",
    "    return mu, sigma\n",
    "\n",
    "# normal distribution function that determines a value below threshold. #2,#3\n",
    "def normal_cdf(x, mu=0, sigma=1):\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "normal_probability_below = normal_cdf\n",
    "\n",
    "# normal distribution that determines a value above threshold #4\n",
    "def normal_probability_above(lo, mu=0, sigma=1):\n",
    "    return 1 - normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# normal distribution functino that determines a value between #5\n",
    "def normal_probability_between(lo, hi, mu=0, sigma=1):\n",
    "    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# normal distribution function that determines a value outside #6\n",
    "def normal_probability_outside(lo, hi, mu=0, sigma=1):\n",
    "    return 1 - normal_probability_between(lo, hi, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By creating functions that find the nontail region of our distribution, we can do the reverse of the above using the `inverse_normal_cdf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverse_normal_cdf(p, mu=0, sigma=1, tolerance=0.00001):\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
    "    low_z, low_p = -10.0, 0\n",
    "    hi_z, hi_p = 10.0, 1\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z) / 2\n",
    "        mid_p = normal_cdf(mid_z)\n",
    "        if mid_p < p:\n",
    "            low_z, low_p = mid_z, mid_p\n",
    "        elif mid_p > p:\n",
    "            hi_z, hi_p = mid_z, mid_p\n",
    "        else:\n",
    "            break\n",
    "    return mid_z\n",
    "\n",
    "def normal_upper_bound(probability, mu=0, sigma=1):\n",
    "    return inverse_normal_cdf(probability, mu, sigma)\n",
    "\n",
    "def normal_lower_bound(probability, mu=0, sigma=1):\n",
    "    return inverse_normal_cdf(1 - probability, mu, sigma)\n",
    "\n",
    "def normal_two_sided_bounds(probability, mu=0, sigma=1):\n",
    "    tail_probability = (1 - probability) / 2\n",
    "    upper_bound = normal_lower_bound(tail_probability, mu, sigma)\n",
    "    lower_bound = normal_upper_bound(tail_probability, mu, sigma)\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've created our functions, let's begin testing.  let `n=1000` where `n` is the number of coin flips that will populate our event data.  If our hypothesis is true, `X` should have a mean close to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0 15.811388300841896\n"
     ]
    }
   ],
   "source": [
    "mu_0, sigma_0 = normal_approximation_to_binomial(1000, 0.5)\n",
    "\n",
    "print(mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've gotten our `mu` (mean) and `sigma` (standard deviation) values.  Next, we'll need to determine significance.  This is done by setting our willingness to accept a false positive at `5%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469.01026640487555, 530.9897335951244)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_two_sided_bounds(0.95, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values 469 and 531 are now considered our lower and upper bounds, respectively.  If `Hsub0` (our hypothesis that a coin flips fairly one way or another) is true, and `p=0.5` is true, then that should mean that our test will only fail 19/20 flips made.\n",
    "\n",
    "Our next goal is to determine the *power* of our test.  While determining significance allows us to find type 1 errors (false positives), power allows us to find type 2 errors (a failure to reject `Hsub0` even though it is false).  To determine this, we must derive a value that `p` should not be.  In this instance, we'll determine that `p=0.55`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469.01026640487555 530.9897335951244\n"
     ]
    }
   ],
   "source": [
    "# set vars for determining power of our test\n",
    "lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)\n",
    "print(lo, hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550.0 15.732132722552274\n"
     ]
    }
   ],
   "source": [
    "# set vars for determining power if p = 0.55\n",
    "mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55)\n",
    "print(mu_1, sigma_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we can determine our power value.  However, there's an issue with the logic of `Hsub1`'s lower bounds.  It could potentially eliminate an `Hsub0` value if the mean falls below 500 since its lower bound is 469, and we know that's not going to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8865480012953671\n"
     ]
    }
   ],
   "source": [
    "type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "print(power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a better power value, we can introduce a one sided test to determine if `X` is larger than 50, but not when it's smaller.  One sided tests are useful when conducting hypothesis tests where `Hsub1` is known to have a bias in one direction versus another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526.0073585242053\n"
     ]
    }
   ],
   "source": [
    "hi = normal_upper_bound(0.95, mu_0, sigma_0)\n",
    "print(hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9363794803307173\n"
     ]
    }
   ],
   "source": [
    "type_2_probability = normal_probability_below(hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "print(power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's a lot better.  This new test now only rejects `Hsub0` when `X` is between 526 (derived from `hi`) and 531 (derived from `sigma_1`).\n",
    "\n",
    "Another way of deriving probability is through the use of *p-values*.  Instead of deriving probability from using thresholds, you can derive the probability computationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06207721579598857"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def two_sided_p_value(x, mu=0, sigma=1):\n",
    "    if x >= mu:\n",
    "        return 2 * normal_probability_above(x, mu, sigma)\n",
    "    else:\n",
    "        return 2 * normal_probability_below(x, mu, sigma)\n",
    "\n",
    "# using 529.5 instead of 530 for continuity correction.  Basically 529.5-530.5 as a range is a better estimate than\n",
    "# using 530 specifically.\n",
    "two_sided_p_value(529.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick way to determine that continuity corrections are an accurate representation of 530 than directly calling 530 is to run a quick simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06181\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "extreme_value_count = 0\n",
    "\n",
    "for _ in range(100000):\n",
    "    num_heads = sum(1 if random.random() < 0.5 else 0\n",
    "                   for _ in range(1000))\n",
    "    if num_heads >=530 or num_heads <=470:\n",
    "        extreme_value_count += 1\n",
    "\n",
    "print(extreme_value_count / 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does this value mean?  Since it's larger than 5%, we don't reject the null hypothesis.  If it was just a bit larger, the outcome would be a bit different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046345287837786575"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(531.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this value falls below our 5% threshold, we would have to reject this null.\n",
    "\n",
    "For a one sided test, we would have the following new functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06062885772582083"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_p_value = normal_probability_above\n",
    "lower_p_value = normal_probability_below\n",
    "\n",
    "upper_p_value(524.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value wouldn't be rejected, but if the value were 527:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04686839508859242"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_p_value(526.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which would be rejected by the one sided test.\n",
    "\n",
    "Another way of determining p values would be through confidence intervals.  By using central limit theorem, we can determine the average of the Bernoulli vars `X` should be normal, with mean `p` and standard deviation:\n",
    "\n",
    "`math.sqrt(p * (1 - p) / 1000)`\n",
    "\n",
    "We don't know `p`, so instead we use an estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015791611697353755\n"
     ]
    }
   ],
   "source": [
    "p_hat  = 525 / 1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4940490278129096, 0.5559509721870904)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, using the normal approximation, we can say that we are 95% confident that the interval contains `p`.\n",
    "\n",
    "Alternatively, a result that would not pass confidence would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015760710643876435\n"
     ]
    }
   ],
   "source": [
    "p_hat = 540 / 1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5091095927295919, 0.5708904072704082)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since this value doesn't pass `Hsub0` it fails confidence.\n",
    "\n",
    "A way to reduce erroneous rejections would be through *p-hacking*.  P-hacking is a process by which a statistician would hack away a proposed null hypotheses, eliminating enough outliers to get a p-value below `0.05`.  While this may be a viable way of determining the accuracy of your results, a good data scientist should have a hypothesis developed prior to reviewing data, and clean the data without consideration to hypothesis.  Additionally, p-values shouldn't be a substitute for common sense.\n",
    "\n",
    "When attempting to compare two sets of data, it may be appropriate to use *A/B tests* to test those comparisons.  In this example, we'll say that we are testing the popularity of two adds A and B.  If `NsubA` people see ad A and `nsubA` people have clicked it, and `NsubB` people see ad A and `nsubB` people have clicked it, we know that `nsubA | NsubA` is approximately a normal random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimated_parameters(N, n):\n",
    "    p = n / N\n",
    "    sigma = math.sqrt(p * (1 - p) / N)\n",
    "    return p, sigma\n",
    "\n",
    "def a_b_test_statistic(N_A, n_A, N_B, n_B):\n",
    "    p_A, sigma_A = estimated_parameters(N_A, n_A)\n",
    "    p_B, sigma_B = estimated_parameters(N_B, n_B)\n",
    "    return (p_B - p_A) / math.sqrt(sigma_A ** 2 + sigma_B ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if Ad A \"Tastes Great\" gets `200 clicks/1000 views` and Ad B \"Less Bias\" gets `180 clicks / 1000 views`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1403464899034472\n"
     ]
    }
   ],
   "source": [
    "z = a_b_test_statistic(1000, 200, 1000, 180)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of seeing such a large difference if the means were actually equal would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.254141976542236"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is large enough that you can't conclude there's much of a difference.  On the other hand, if \"Less Bias\" only got 150 clicks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.948839123097944\n"
     ]
    }
   ],
   "source": [
    "z = a_b_test_statistic(1000, 200, 1000, 150)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003189699706216853"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means there's only a 0.003 probability that you'd see such a large difference if the ads were equally effective.\n",
    "\n",
    "a final method of of determining the validity of a hypothesis is by treating the unknown parameters themselves as random variables.  By using a *Prior distribution* for the parameters and then using the observed data and *Bayes's Theorem* to get an updated *posterior distribution* for the parameters, you can make probability judgements about the parameters themselves instead of the tests.\n",
    "\n",
    "For example, when the unknown parameter is a probability like in the coin flipping example, we often use a prior from the *Beta distribution*, which puts all its probability between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def B(alpha, beta):\n",
    "    return math.gamma(alpha) + math.gamma(beta) / math.gamma(alpha + beta)\n",
    "\n",
    "def beta_pdf(x, alpha, beta):\n",
    "    if x < 0 or x > 1:\n",
    "        return 0\n",
    "    return x ** (alpha - 1) * (1 - x) ** (beta - 1) / B(alpha, beta)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
