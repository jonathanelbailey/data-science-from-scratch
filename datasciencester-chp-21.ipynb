{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many interesting data problems can be fruitfully thought of in terms of networks, consisting of nodes of some type nand the edges that join them.  For instance, your facebook friends form the nodes of a network whose edges are friendship relations.  A less obvious example is the world wide web itself, with each web page a node, and each hyperlink from one page to another an edge.\n",
    "\n",
    "Facebook friendship is mutual -- If I am facebook friends with you, then necessarily you are friends with me.  In this case, we say that the edges are undirected.  Hyperlinks are not -- my website links to whitehouse.gov, but whitehouse.gov refuses to link to my website.  We call these types of edges directed.  We'll look at both kinds of networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, re\n",
    "from collections import defaultdict, Counter, deque\n",
    "from code_python3.linear_algebra import dot, get_row, get_column, make_matrix, magnitude, scalar_multiply, shape, distance\n",
    "from functools import partial\n",
    "\n",
    "users = [\n",
    "    { \"id\": 0, \"name\": \"Hero\" },\n",
    "    { \"id\": 1, \"name\": \"Dunn\" },\n",
    "    { \"id\": 2, \"name\": \"Sue\" },\n",
    "    { \"id\": 3, \"name\": \"Chi\" },\n",
    "    { \"id\": 4, \"name\": \"Thor\" },\n",
    "    { \"id\": 5, \"name\": \"Clive\" },\n",
    "    { \"id\": 6, \"name\": \"Hicks\" },\n",
    "    { \"id\": 7, \"name\": \"Devin\" },\n",
    "    { \"id\": 8, \"name\": \"Kate\" },\n",
    "    { \"id\": 9, \"name\": \"Klein\" }\n",
    "]\n",
    "\n",
    "friendships = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\n",
    "               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\n",
    "\n",
    "# give each user a friends list\n",
    "for user in users:\n",
    "    user[\"friends\"] = []\n",
    "\n",
    "# and populate it\n",
    "for i, j in friendships:\n",
    "    # this works because users[i] is the user whose id is i\n",
    "    users[i][\"friends\"].append(users[j]) # add i as a friend of j\n",
    "    users[j][\"friends\"].append(users[i]) # add j as a friend of i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were dissatisfied with our notion of degree centrality, which didn't really agree with our intuitions about who were the key connectors of the network.  An alternative metric is betweenness centrality, which identifies people who frequently are on the shortest paths between pairs of other people.  In particular, the betweenness centrality of node `i` is computed by adding up for every other pair of nodes `j` and `k`, the proportion of shortest paths between node j and node k that pass through i.\n",
    "\n",
    "So, as a first step, we'll need to figure out the shortest paths between all pairs of people.  There are some pretty sophisticated algorithms for doing so efficiently, but we will use a less efficient, easier to understand algorithm.  This algorithm (an implementation of breadth-first search) is a bit more complicated than the others:\n",
    "\n",
    "1. Our goal is a function that takes a `from_user` and finds all shortest paths to every other user.\n",
    "2. We'll represent a path as list of user IDs.  Since every path starts at `from_user`, we won't include her ID in the list.  This means that the length of the list representing the path will be the length of the path itself.\n",
    "3. We'll maintain a dictionary `shortest_paths_to` where the keys are user IDs and the values are lists of paths that end at the user with the specified ID.  If there is a unique shortest path, the list will just contain that one path.  If there are multiple shortest paths, the list will contain all of them.\n",
    "4. We'll also maintain a queue `frontier` that contains the users we want to explore in the order we want to explore them.  We'll store them as pairs `(prev_user, user)`  so that we know how we got to each one.  We initialize the queue with all the neighbors of `from_user`.\n",
    "5. As we explore the graph, whenever we find new neighbors that we don't already know shortest paths to, we add them to the end of the queue to explore later, with the current user as `prev_user`.\n",
    "6. When we take a user off the queue, and we've never encountered that user before, we've definitely found one or more shortest path to him -- each of the shortest paths to `prev_user` with one extra step added.\n",
    "7. When we take a user off the queue and we have encountered that user before, then either we've found another shortest path or we've found a longer path.\n",
    "8. When no more users are left on the queue, we've explored the ewhole graph and we're done.\n",
    "\n",
    "We can put this altogether like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_paths_from(from_user):\n",
    "\n",
    "    # a dictionary from \"user_id\" to *all* shortest paths to that user\n",
    "    shortest_paths_to = { from_user[\"id\"] : [[]] }\n",
    "\n",
    "    # a queue of (previous user, next user) that we need to check.\n",
    "    # starts out with all pairs (from_user, friend_of_from_user)\n",
    "    frontier = deque((from_user, friend)\n",
    "                     for friend in from_user[\"friends\"])\n",
    "\n",
    "    # keep going until we empty the queue\n",
    "    while frontier:\n",
    "\n",
    "        prev_user, user = frontier.popleft() # take from the beginning\n",
    "        user_id = user[\"id\"]\n",
    "\n",
    "        # the fact that we're pulling from our queue means that\n",
    "        # necessarily we already know a shortest path to prev_user\n",
    "        paths_to_prev = shortest_paths_to[prev_user[\"id\"]]\n",
    "        paths_via_prev = [path + [user_id] for path in paths_to_prev]\n",
    "\n",
    "        # it's possible we already know a shortest path to here as well\n",
    "        old_paths_to_here = shortest_paths_to.get(user_id, [])\n",
    "\n",
    "        # what's the shortest path to here that we've seen so far?\n",
    "        if old_paths_to_here:\n",
    "            min_path_length = len(old_paths_to_here[0])\n",
    "        else:\n",
    "            min_path_length = float('inf')\n",
    "\n",
    "        # any new paths to here that aren't too long\n",
    "        new_paths_to_here = [path_via_prev\n",
    "                             for path_via_prev in paths_via_prev\n",
    "                             if len(path_via_prev) <= min_path_length\n",
    "                             and path_via_prev not in old_paths_to_here]\n",
    "\n",
    "        shortest_paths_to[user_id] = old_paths_to_here + new_paths_to_here\n",
    "\n",
    "        # add new neighbors to the frontier\n",
    "        frontier.extend((user, friend)\n",
    "                        for friend in user[\"friends\"]\n",
    "                        if friend[\"id\"] not in shortest_paths_to)\n",
    "\n",
    "    return shortest_paths_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can store these `dicts` with each node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    user[\"shortest_paths\"] = shortest_paths_from(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're finally ready to compute betweenness centrality.  For every pair of nodes i and j, we know the n shortest paths from i to j.  Then, for each of those paths, we just add 1/n to the centrality of each node on that path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betweenness Centrality\n",
      "0 0.0\n",
      "1 3.5\n",
      "2 3.5\n",
      "3 18.0\n",
      "4 20.0\n",
      "5 20.5\n",
      "6 6.0\n",
      "7 6.0\n",
      "8 8.5\n",
      "9 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for user in users:\n",
    "    user[\"betweenness_centrality\"] = 0.0\n",
    "\n",
    "for source in users:\n",
    "    source_id = source[\"id\"]\n",
    "    for target_id, paths in source[\"shortest_paths\"].items():\n",
    "        if source_id < target_id:   # don't double count\n",
    "            num_paths = len(paths)  # how many shortest paths?\n",
    "            contrib = 1 / num_paths # contribution to centrality\n",
    "            for path in paths:\n",
    "                for id in path:\n",
    "                    if id not in [source_id, target_id]:\n",
    "                        users[id][\"betweenness_centrality\"] += contrib\n",
    "                        \n",
    "print(\"Betweenness Centrality\")\n",
    "for user in users:\n",
    "    print(user[\"id\"], user[\"betweenness_centrality\"])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another measure we can look at is closeness centrality.  First, for each user we compute her farness, which is the sum of the lengths of her shortest paths to each other user.  Since we've already computed the shortest paths between each pair of nodes, it's easy to add their lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closeness Centrality\n",
      "0 0.029411764705882353\n",
      "1 0.037037037037037035\n",
      "2 0.037037037037037035\n",
      "3 0.045454545454545456\n",
      "4 0.05\n",
      "5 0.05\n",
      "6 0.041666666666666664\n",
      "7 0.041666666666666664\n",
      "8 0.03571428571428571\n",
      "9 0.027777777777777776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def farness(user):\n",
    "    \"\"\"the sum of the lengths of the shortest paths to each other user\"\"\"\n",
    "    return sum(len(paths[0])\n",
    "               for paths in user[\"shortest_paths\"].values())\n",
    "\n",
    "for user in users:\n",
    "    user[\"closeness_centrality\"] = 1 / farness(user)\n",
    "    \n",
    "print(\"Closeness Centrality\")\n",
    "for user in users:\n",
    "    print(user[\"id\"], user[\"closeness_centrality\"])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is much less variation here.  Even the very central nodes are still pretty far from the nodes out on the periphery.  For larger networks though, Eigenvector Centrality is used.\n",
    "\n",
    "In order to talk aboout eigenvector centrality, we have to talk about eigenvectors.  And in order to talk about eigenvectors, we have to talk about matrix multiplication.  If A is a $ n_1 x k_1 $ and B is a $ n_2 x k_2 $ matrix, and if $ k_1 = n_2 $, then their produict AB is the $ n_1 x k_2 $ matrix whose (i,j)th entry is:\n",
    "\n",
    "$$ A_ilB_kj + A_i2 B_2j + ... + A_ik B_kj $$\n",
    "\n",
    "Which is just the `dot` product of the *i*th row of A (thought of as a vector) with the *j*th column of B (also thought of as a vector):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_product_entry(A, B, i, j):\n",
    "    return dot(get_row(A, i), get_column(B, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After which we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_multiply(A, B):\n",
    "    n1, k1 = shape(A)\n",
    "    n2, k2 = shape(B)\n",
    "    if k1 != n2:\n",
    "        raise ArithmeticError(\"incompatible shapes!\")\n",
    "\n",
    "    return make_matrix(n1, k2, partial(matrix_product_entry, A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if A is an $ n x k $ matrix and B is a $ k * 1 $ matrix, then AB is an $ n * 1 $ matrix.  If we treat a vector as a one-column matrix, we can think of A as a function that maps k-dimensional vectors to n-dimensional vectors, where the function is just matrix multiplication.\n",
    "\n",
    "Previously, we represented  vectors simply as lists, which isn't quite the same, so we'll need to create some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_as_matrix(v):\n",
    "    \"\"\"returns the vector v (represented as a list) as a n x 1 matrix\"\"\"\n",
    "    return [[v_i] for v_i in v]\n",
    "\n",
    "def vector_from_matrix(v_as_matrix):\n",
    "    \"\"\"returns the n x 1 matrix as a list of values\"\"\"\n",
    "    return [row[0] for row in v_as_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When A is a square matrix, this operation maps n-dimensional vectors to other n-diumensional vectors.  It's possible that for some matrix A and vector v, when A operates on v we get back a scalar multiple of v.  That is, that the result is a vector that points in the same direction as v.  When this happens (and when, in addition, v is not a vector of all zeroes), we call v an eigenvector of A.  And we call the multiplier an eigenvalue.\n",
    "\n",
    "One possible way to find an eigenvector of A is b y picking a starting vector v, applying `matrix_operate`, rescaling the result to have magnitude 1, and repeating until the process converges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_operate(A, v):\n",
    "    v_as_matrix = vector_as_matrix(v)\n",
    "    product = matrix_multiply(A, v_as_matrix)\n",
    "    return vector_from_matrix(product)\n",
    "\n",
    "def find_eigenvector(A, tolerance=0.00001):\n",
    "    guess = [1 for __ in A]\n",
    "\n",
    "    while True:\n",
    "        result = matrix_operate(A, guess)\n",
    "        length = magnitude(result)\n",
    "        next_guess = scalar_multiply(1/length, result)\n",
    "\n",
    "        if distance(guess, next_guess) < tolerance:\n",
    "            return next_guess, length # eigenvector, eigenvalue\n",
    "\n",
    "        guess = next_guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By construction, the returned `guess` is a vector such that, when you apply `matrix_operate` to it and rescale it to have length 1, you get back itself.  Which means it's an eigenvector.\n",
    "\n",
    "Not all matrices of real numbers have eigenvectors and eigenvalues.  for example the matrix:\n",
    "\n",
    "```python\n",
    "rotate = [[ 0, 1],\n",
    "          [ -1, 0]]\n",
    "```\n",
    "\n",
    "rotates vectors 90 degrees clockwise, which means that the only vector it maps to a scalar multple of itself is a vector of zeroes.  If you tried `find_eigenvector(rotate)` it would run forever.  Even matrices that have eigenvectors can sometimes get stuck in cycles.  Consider the matrix:\n",
    "\n",
    "```python\n",
    "flip = [[ 0, 1],\n",
    "        [1, 0]]\n",
    "```\n",
    "\n",
    "This matrix maps any vector to its inverse.  It will run forever, but when it does stop it will return an eigenvector.  So how does this help us understand our network?  To start with, we'll need to represent the connections in our network as an `adjacency_matrix`, whose (i,j)th entry is either 1 or 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector Centrality\n",
      "0 0.38578006614957344\n",
      "1 0.5147902322356226\n",
      "2 0.5147902322356226\n",
      "3 0.47331220396377677\n",
      "4 0.23361029944966002\n",
      "5 0.1501458150031844\n",
      "6 0.08355561051056493\n",
      "7 0.08355561051056493\n",
      "8 0.07284034177922594\n",
      "9 0.027294660139652423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def entry_fn(i, j):\n",
    "    return 1 if (i, j) in friendships or (j, i) in friendships else 0\n",
    "\n",
    "n = len(users)\n",
    "adjacency_matrix = make_matrix(n, n, entry_fn)\n",
    "\n",
    "eigenvector_centralities, _ = find_eigenvector(adjacency_matrix)\n",
    "\n",
    "print(\"Eigenvector Centrality\")\n",
    "for user_id, centrality in enumerate(eigenvector_centralities):\n",
    "    print(user_id, centrality)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvector centrality for each user is then the entry corresponding to that user in the eigenvector returned  by `find_eigenvector`.  Users with high eigenvector centrality should be those who have a lot of connections and connections to people who themselves have high centrality.  Here users 1 and 2 are the most central, as they both have three conections to people who are themselves highly central. As we move away from them, people's centralities steadily drop off.\n",
    "\n",
    "On a network this small, eigenvector centrality behaves somewhat erratically.  If you tryn adding or subtracting links, you'll find that small changes in the network can dramatically change the centrality numbers.  In a much larger network this would not particularly be the case.\n",
    "\n",
    "We still haven't motivated why an eigenvector might lead to a reasonable notion of centrality.  Being an eigenvector means that if you compute:\n",
    "\n",
    "```python\n",
    "matrix_operate(adjacency_matrix, eigenvector_centralities)\n",
    "```\n",
    "\n",
    "this results in a scalar multiple of `eigenvector_centralities`.  If you look at how matrix multiplication works, `matrix_operate` produces a vector whose *i*th element is:\n",
    "\n",
    "```python\n",
    "dot(get_row(adjacency_matrix, i), eigenvector_centralities)\n",
    "```\n",
    "\n",
    "Which is precisely the sum of the eigenvector centralities of the users connected to user *i*.  In other words, eigenvector centralities are numbers, one per user, such that each user's value is a constant multiple of the sum of his neighbors' values.  In this case, centrality means being connected to people who themselves are central.  The more centrality you are directly connected to, the more central you are.  This is of course a circular definition; eigenvectors are the way of breaking out of the circularity.\n",
    "\n",
    "Another way of understanding this is by thinking about what `find_eigenvector` is doing here.  It starts by assigning each node a random centrality.  It them repeats the following two steps until the process converges:\n",
    "\n",
    "1. Give each node a new centrality score that equals the sum of its neighbors' old centrality scores.\n",
    "2. Rescale the vector of centralities to have magnitude 1.\n",
    "\n",
    "Although the mathematics behind it may seem somewhat opaque at first, the calculation itse;f is relatively straightforward and is pretty easy to perform on even very large graphs.\n",
    "\n",
    "We'll track endorsements `(source, target)` that no longer represent a reciprocal relationship, but rather that source endorses `target` as an awesome data scientist.  We'll need to account for this assemetry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "endorsements = [(0, 1), (1, 0), (0, 2), (2, 0), (1, 2), (2, 1), (1, 3),\n",
    "                (2, 3), (3, 4), (5, 4), (5, 6), (7, 5), (6, 8), (8, 7), (8, 9)]\n",
    "\n",
    "for user in users:\n",
    "    user[\"endorses\"] = []       # add one list to track outgoing endorsements\n",
    "    user[\"endorsed_by\"] = []    # and another to track endorsements\n",
    "\n",
    "for source_id, target_id in endorsements:\n",
    "    users[source_id][\"endorses\"].append(users[target_id])\n",
    "    users[target_id][\"endorsed_by\"].append(users[source_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After which we can easily find the `most_endorsed` data scientists and sell that information to recruiters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (3, 2),\n",
       " (4, 2),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endorsements_by_id = [(user[\"id\"], len(user[\"endorsed_by\"]))\n",
    "                      for user in users]\n",
    "\n",
    "sorted(endorsements_by_id,\n",
    "       key=lambda pair: pair[1],\n",
    "       reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, \"number of endorsements\" is an easy metric to game.  All you need to do is create phony accounts and have them endorse you.  Or arrange with your friends to endorse each other.  A better metric would take into account *who* endorses you.  Endorsements from people who have a lot of endorsements should somehow count more than endorsements from people with few endorsements.  This is the essence of the PageRank algorithm, used by Google to rank websites based on which other websites link to them, which other websites link to those, and so on.\n",
    "\n",
    "A simplified version looks like:\n",
    "\n",
    "1. There is a total of 1.0 (or 100%) PageRank in the network.\n",
    "2. Initially this PageRank is equally distributed among nodes.\n",
    "3. At each step, a large fraction of each node's PageRank is distributed evenly among jits outgoing links.\n",
    "4. At each step, the remainder of each node's PageRank is distributed evenly among all nodes.\n",
    "\n",
    "Here's what that code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank\n",
      "0 0.0404553415061296\n",
      "1 0.044921190893169885\n",
      "2 0.044921190893169885\n",
      "3 0.0404553415061296\n",
      "4 0.06785083675770529\n",
      "5 0.04344422700587085\n",
      "6 0.03346379647749512\n",
      "7 0.03346379647749512\n",
      "8 0.04344422700587085\n",
      "9 0.03346379647749512\n"
     ]
    }
   ],
   "source": [
    "def page_rank(users, damping = 0.85, num_iters = 100):\n",
    "\n",
    "    # initially distribute PageRank evenly\n",
    "    num_users = len(users)\n",
    "    pr = { user[\"id\"] : 1 / num_users for user in users }\n",
    "\n",
    "    # this is the small fraction of PageRank\n",
    "    # that each node gets each iteration\n",
    "    base_pr = (1 - damping) / num_users\n",
    "\n",
    "    for __ in range(num_iters):\n",
    "        next_pr = { user[\"id\"] : base_pr for user in users }\n",
    "        for user in users:\n",
    "            # distribute PageRank to outgoing links\n",
    "            links_pr = pr[user[\"id\"]] * damping\n",
    "            for endorsee in user[\"endorses\"]:\n",
    "                next_pr[endorsee[\"id\"]] += links_pr / len(user[\"endorses\"])\n",
    "\n",
    "        pr = next_pr\n",
    "\n",
    "    return pr\n",
    "\n",
    "print(\"PageRank\")\n",
    "for user_id, pr in page_rank(users).items():\n",
    "    print(user_id, pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though he has few endorsements than users 0, 1, and 2, his endorsements carry with them rank from their endorsements.  Additionally, both of his endorsers endorsed only him, which means that he doesn't have to divide their rank with anyone else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
