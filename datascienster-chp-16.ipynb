{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an anonymized data set of about 200 users.  Each data point contains salary, years of experience as a data scientist, and whether she paid for a premium account as categorical variables.  We represent the dependent variables as either 0 or 1.  Our data is in a matrix where each row is a list.  Let's turn it into the format we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x data:\n",
      "[[1, 0.7, 48000], [1, 1.9, 48000], [1, 2.5, 60000], [1, 4.2, 63000], [1, 6, 76000], [1, 6.5, 69000], [1, 7.5, 76000], [1, 8.1, 88000], [1, 8.7, 83000], [1, 10, 83000], [1, 0.8, 43000], [1, 1.8, 60000], [1, 10, 79000], [1, 6.1, 76000], [1, 1.4, 50000], [1, 9.1, 92000], [1, 5.8, 75000], [1, 5.2, 69000], [1, 1, 56000], [1, 6, 67000], [1, 4.9, 74000], [1, 6.4, 63000], [1, 6.2, 82000], [1, 3.3, 58000], [1, 9.3, 90000], [1, 5.5, 57000], [1, 9.1, 102000], [1, 2.4, 54000], [1, 8.2, 65000], [1, 5.3, 82000], [1, 9.8, 107000], [1, 1.8, 64000], [1, 0.6, 46000], [1, 0.8, 48000], [1, 8.6, 84000], [1, 0.6, 45000], [1, 0.5, 30000], [1, 7.3, 89000], [1, 2.5, 48000], [1, 5.6, 76000], [1, 7.4, 77000], [1, 2.7, 56000], [1, 0.7, 48000], [1, 1.2, 42000], [1, 0.2, 32000], [1, 4.7, 56000], [1, 2.8, 44000], [1, 7.6, 78000], [1, 1.1, 63000], [1, 8, 79000], [1, 2.7, 56000], [1, 6, 52000], [1, 4.6, 56000], [1, 2.5, 51000], [1, 5.7, 71000], [1, 2.9, 65000], [1, 1.1, 33000], [1, 3, 62000], [1, 4, 71000], [1, 2.4, 61000], [1, 7.5, 75000], [1, 9.7, 81000], [1, 3.2, 62000], [1, 7.9, 88000], [1, 4.7, 44000], [1, 2.5, 55000], [1, 1.6, 41000], [1, 6.7, 64000], [1, 6.9, 66000], [1, 7.9, 78000], [1, 8.1, 102000], [1, 5.3, 48000], [1, 8.5, 66000], [1, 0.2, 56000], [1, 6, 69000], [1, 7.5, 77000], [1, 8, 86000], [1, 4.4, 68000], [1, 4.9, 75000], [1, 1.5, 60000], [1, 2.2, 50000], [1, 3.4, 49000], [1, 4.2, 70000], [1, 7.7, 98000], [1, 8.2, 85000], [1, 5.4, 88000], [1, 0.1, 46000], [1, 1.5, 37000], [1, 6.3, 86000], [1, 3.7, 57000], [1, 8.4, 85000], [1, 2, 42000], [1, 5.8, 69000], [1, 2.7, 64000], [1, 3.1, 63000], [1, 1.9, 48000], [1, 10, 72000], [1, 0.2, 45000], [1, 8.6, 95000], [1, 1.5, 64000], [1, 9.8, 95000], [1, 5.3, 65000], [1, 7.5, 80000], [1, 9.9, 91000], [1, 9.7, 50000], [1, 2.8, 68000], [1, 3.6, 58000], [1, 3.9, 74000], [1, 4.4, 76000], [1, 2.5, 49000], [1, 7.2, 81000], [1, 5.2, 60000], [1, 2.4, 62000], [1, 8.9, 94000], [1, 2.4, 63000], [1, 6.8, 69000], [1, 6.5, 77000], [1, 7, 86000], [1, 9.4, 94000], [1, 7.8, 72000], [1, 0.2, 53000], [1, 10, 97000], [1, 5.5, 65000], [1, 7.7, 71000], [1, 8.1, 66000], [1, 9.8, 91000], [1, 8, 84000], [1, 2.7, 55000], [1, 2.8, 62000], [1, 9.4, 79000], [1, 2.5, 57000], [1, 7.4, 70000], [1, 2.1, 47000], [1, 5.3, 62000], [1, 6.3, 79000], [1, 6.8, 58000], [1, 5.7, 80000], [1, 2.2, 61000], [1, 4.8, 62000], [1, 3.7, 64000], [1, 4.1, 85000], [1, 2.3, 51000], [1, 3.5, 58000], [1, 0.9, 43000], [1, 0.9, 54000], [1, 4.5, 74000], [1, 6.5, 55000], [1, 4.1, 41000], [1, 7.1, 73000], [1, 1.1, 66000], [1, 9.1, 81000], [1, 8, 69000], [1, 7.3, 72000], [1, 3.3, 50000], [1, 3.9, 58000], [1, 2.6, 49000], [1, 1.6, 78000], [1, 0.7, 56000], [1, 2.1, 36000], [1, 7.5, 90000], [1, 4.8, 59000], [1, 8.9, 95000], [1, 6.2, 72000], [1, 6.3, 63000], [1, 9.1, 100000], [1, 7.3, 61000], [1, 5.6, 74000], [1, 0.5, 66000], [1, 1.1, 59000], [1, 5.1, 61000], [1, 6.2, 70000], [1, 6.6, 56000], [1, 6.3, 76000], [1, 6.5, 78000], [1, 5.1, 59000], [1, 9.5, 74000], [1, 4.5, 64000], [1, 2, 54000], [1, 1, 52000], [1, 4, 69000], [1, 6.5, 76000], [1, 3, 60000], [1, 4.5, 63000], [1, 7.8, 70000], [1, 3.9, 60000], [1, 0.8, 51000], [1, 4.2, 78000], [1, 1.1, 54000], [1, 6.2, 60000], [1, 2.9, 59000], [1, 2.1, 52000], [1, 8.2, 87000], [1, 4.8, 73000], [1, 2.2, 42000], [1, 9.1, 98000], [1, 6.5, 84000], [1, 6.9, 73000], [1, 5.1, 72000], [1, 9.1, 69000], [1, 9.8, 79000]]\n",
      "y data:\n",
      "[1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from functools import partial, reduce\n",
    "from code_python3.linear_algebra import dot, vector_add\n",
    "from code_python3.gradient_descent import maximize_stochastic, maximize_batch\n",
    "from code_python3.working_with_data import rescale\n",
    "from code_python3.machine_learning import train_test_split\n",
    "from code_python3.multiple_regression import estimate_beta, predict\n",
    "import math, random\n",
    "\n",
    "data = [(0.7,48000,1),(1.9,48000,0),(2.5,60000,1),(4.2,63000,0),(6,76000,0),(6.5,69000,0),(7.5,76000,0),(8.1,88000,0),(8.7,83000,1),(10,83000,1),(0.8,43000,0),(1.8,60000,0),(10,79000,1),(6.1,76000,0),(1.4,50000,0),(9.1,92000,0),(5.8,75000,0),(5.2,69000,0),(1,56000,0),(6,67000,0),(4.9,74000,0),(6.4,63000,1),(6.2,82000,0),(3.3,58000,0),(9.3,90000,1),(5.5,57000,1),(9.1,102000,0),(2.4,54000,0),(8.2,65000,1),(5.3,82000,0),(9.8,107000,0),(1.8,64000,0),(0.6,46000,1),(0.8,48000,0),(8.6,84000,1),(0.6,45000,0),(0.5,30000,1),(7.3,89000,0),(2.5,48000,1),(5.6,76000,0),(7.4,77000,0),(2.7,56000,0),(0.7,48000,0),(1.2,42000,0),(0.2,32000,1),(4.7,56000,1),(2.8,44000,1),(7.6,78000,0),(1.1,63000,0),(8,79000,1),(2.7,56000,0),(6,52000,1),(4.6,56000,0),(2.5,51000,0),(5.7,71000,0),(2.9,65000,0),(1.1,33000,1),(3,62000,0),(4,71000,0),(2.4,61000,0),(7.5,75000,0),(9.7,81000,1),(3.2,62000,0),(7.9,88000,0),(4.7,44000,1),(2.5,55000,0),(1.6,41000,0),(6.7,64000,1),(6.9,66000,1),(7.9,78000,1),(8.1,102000,0),(5.3,48000,1),(8.5,66000,1),(0.2,56000,0),(6,69000,0),(7.5,77000,0),(8,86000,0),(4.4,68000,0),(4.9,75000,0),(1.5,60000,0),(2.2,50000,0),(3.4,49000,1),(4.2,70000,0),(7.7,98000,0),(8.2,85000,0),(5.4,88000,0),(0.1,46000,0),(1.5,37000,0),(6.3,86000,0),(3.7,57000,0),(8.4,85000,0),(2,42000,0),(5.8,69000,1),(2.7,64000,0),(3.1,63000,0),(1.9,48000,0),(10,72000,1),(0.2,45000,0),(8.6,95000,0),(1.5,64000,0),(9.8,95000,0),(5.3,65000,0),(7.5,80000,0),(9.9,91000,0),(9.7,50000,1),(2.8,68000,0),(3.6,58000,0),(3.9,74000,0),(4.4,76000,0),(2.5,49000,0),(7.2,81000,0),(5.2,60000,1),(2.4,62000,0),(8.9,94000,0),(2.4,63000,0),(6.8,69000,1),(6.5,77000,0),(7,86000,0),(9.4,94000,0),(7.8,72000,1),(0.2,53000,0),(10,97000,0),(5.5,65000,0),(7.7,71000,1),(8.1,66000,1),(9.8,91000,0),(8,84000,0),(2.7,55000,0),(2.8,62000,0),(9.4,79000,0),(2.5,57000,0),(7.4,70000,1),(2.1,47000,0),(5.3,62000,1),(6.3,79000,0),(6.8,58000,1),(5.7,80000,0),(2.2,61000,0),(4.8,62000,0),(3.7,64000,0),(4.1,85000,0),(2.3,51000,0),(3.5,58000,0),(0.9,43000,0),(0.9,54000,0),(4.5,74000,0),(6.5,55000,1),(4.1,41000,1),(7.1,73000,0),(1.1,66000,0),(9.1,81000,1),(8,69000,1),(7.3,72000,1),(3.3,50000,0),(3.9,58000,0),(2.6,49000,0),(1.6,78000,0),(0.7,56000,0),(2.1,36000,1),(7.5,90000,0),(4.8,59000,1),(8.9,95000,0),(6.2,72000,0),(6.3,63000,0),(9.1,100000,0),(7.3,61000,1),(5.6,74000,0),(0.5,66000,0),(1.1,59000,0),(5.1,61000,0),(6.2,70000,0),(6.6,56000,1),(6.3,76000,0),(6.5,78000,0),(5.1,59000,0),(9.5,74000,1),(4.5,64000,0),(2,54000,0),(1,52000,0),(4,69000,0),(6.5,76000,0),(3,60000,0),(4.5,63000,0),(7.8,70000,0),(3.9,60000,1),(0.8,51000,0),(4.2,78000,0),(1.1,54000,0),(6.2,60000,0),(2.9,59000,0),(2.1,52000,0),(8.2,87000,0),(4.8,73000,0),(2.2,42000,1),(9.1,98000,0),(6.5,84000,0),(6.9,73000,0),(5.1,72000,0),(9.1,69000,1),(9.8,79000,1),]\n",
    "data = list(map(list, data)) # change tuples to lists\n",
    "\n",
    "x = [[1] + row[:2] for row in data] # each element is [1, experience, salary]\n",
    "y = [row[2] for row in data]        # each element is paid_account\n",
    "\n",
    "print(\"x data:\")\n",
    "print(x)\n",
    "print(\"y data:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first attempt is to use linear regressino and find the best model:\n",
    "\n",
    "$$ paid account = \\beta_\\alpha + \\beta_1experience + \\beta_2salary + \\epsilon $$\n",
    "\n",
    "So there's nothing that could stop us from modeling this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGRZJREFUeJzt3X20XXV95/H3N5cbSOpDQhOnkgcSbMQBRKO3iNBpQXF4cEnwgQoja7RjzbLWp7HDGlw41MHVZWs6Y+uUail1fGJAZFpMLZhSRVvRUC4iIGg0BiGXMCVVglWDhPCdP86+P09Ozrln35uz70nI+7XWXXc//PbvfO9vn3s/d599zt6RmUiSBDBn2AVIkvYfhoIkqTAUJEmFoSBJKgwFSVJhKEiSCkNBklQYCpKkwlCQJBWHDLuA6Vq0aFGuWLFi2GVI0gHltttu+5fMXNyv3QEXCitWrGB8fHzYZUjSASUi7qvTzpePJEmFoSBJKgwFSVJhKEiSCkNBklQYCpKkwlCQJBWNhUJEfDQiHoqIb/ZYHxHxoYjYHBF3RsQLmqpFklRPkx9e+xjwp8Aneqw/E1hVfb0I+HD1XU8y193+AOs2bGLbjp0csWAeF55+NOesXtJ3fb/tplvDf/+bu3n4p7sAWDBvlPeefSxAeYwF80fJhEd27trr8d5z3V1cecv9tN/SfOH8UX7vFcdyzuolvOe6u7jqlq3szmQkgvNftAyAKzfez1R3QV84f5SXH/9M/vrrD/CTx3bvsS6g57aT9bePx+v+4mvc/L0flvmTn3U4544t7zuGnWPTbnQOPJ4w1a3cl3T0O8j9drAbxlhGTrW397XziBXA5zLzuC7r/hz4UmZeVc1vAk7JzAen6nNsbCz9RPOB47rbH+Ddf3UXO3f9/A/evNER3v+q55Y//N3Wv/qFS/i/tz3Qc7vp1nDhtXewa/eez/U5wMhI7LW88/HG7/shn9p4f9c2oyPBCSsW7vHHeLaMzgnWnfs8zlm9ZK9A6KVzDHuNzXRN9gtMub9VX7/fnemKiNsyc6xfu2GeU1gCbG2bn6iW6Ulk3YZNezypAXbu2s26DZumXH/VLVun3G66NXT7o/cETPnHcPLxrrpla882u3bnUAIBYNcTWcajbg2dY9hrbKZrst9++1v1DWssh3nto+iyrOuzMyLWAmsBli9f3mRNGrBtO3ZOubzX+t09jmB7tZ9JDXW3be5Yet/t63jsy9hMp5ZBPs7Bot/vTlOGeaQwASxrm18KbOvWMDMvz8yxzBxbvLjvRf60Hzliwbwpl/daPxLd/mfo3X4mNdTdtlct+4N9HY99GZtu/fbb36pvWGM5zFBYD/zH6l1IJwKP9DufoAPPhacfzbzRkT2WzRsd4cLTj55y/fkvWjbldtOtYXRk7z/sc6Dr8s7Hmzxp3M3oSHDysw6fdk2DMDonynjUraFzDHuNzXRN9ttvf6u+YY1lYy8fRcRVwCnAooiYAH4PGAXIzI8A1wNnAZuBnwK/2VQtGp7JE2K93kEx1fqxIw8fyDsvJreZ6buPJrffn999dOWbXjyjdx91G5t2M3n3EfTe36qv3+9OUxp991ETfPeRJE3fgfDuI0nSfsZQkCQVhoIkqTAUJEmFoSBJKgwFSVJhKEiSCkNBklQYCpKkwlCQJBWGgiSpMBQkSYWhIEkqDAVJUmEoSJIKQ0GSVBgKkqTCUJAkFYaCJKkwFCRJhaEgSSoMBUlSYShIkgpDQZJUGAqSpMJQkCQVhoIkqTAUJEmFoSBJKgwFSVLRaChExBkRsSkiNkfERV3WL4+ImyLi9oi4MyLOarIeSdLUGguFiBgBLgPOBI4Bzo+IYzqavQe4JjNXA+cBf9ZUPZKk/po8UjgB2JyZWzLzMeBqYE1HmwSeVk0/HdjWYD2SpD6aDIUlwNa2+YlqWbv3AhdExARwPfC2bh1FxNqIGI+I8e3btzdRqySJZkMhuizLjvnzgY9l5lLgLOCTEbFXTZl5eWaOZebY4sWLGyhVkgTNhsIEsKxtfil7vzz0RuAagMz8GnAYsKjBmiRJU2gyFG4FVkXEyoiYS+tE8vqONvcDLwWIiH9LKxR8fUiShqSxUMjMx4G3AhuAb9F6l9HdEXFpRJxdNftd4E0RcQdwFfCGzOx8iUmSNEsOabLzzLye1gnk9mWXtE3fA5zcZA2SpPr8RLMkqTAUJEmFoSBJKgwFSVJhKEiSCkNBklQYCpKkwlCQJBWGgiSpMBQkSYWhIEkqDAVJUmEoSJIKQ0GSVBgKkqTCUJAkFYaCJKkwFCRJhaEgSSoMBUlSYShIkgpDQZJUGAqSpMJQkCQVhoIkqTAUJEmFoSBJKgwFSVJhKEiSikZDISLOiIhNEbE5Ii7q0eY3IuKeiLg7Iv5Pk/VIkqZ2SFMdR8QIcBnwMmACuDUi1mfmPW1tVgHvBk7OzIcj4hlN1SNJ6q/JI4UTgM2ZuSUzHwOuBtZ0tHkTcFlmPgyQmQ81WI8kqY8mQ2EJsLVtfqJa1u7ZwLMj4uaI2BgRZzRYjySpj8ZePgKiy7Ls8virgFOApcA/RsRxmbljj44i1gJrAZYvXz74SiVJQLNHChPAsrb5pcC2Lm0+m5m7MvNeYBOtkNhDZl6emWOZObZ48eLGCpakg12ToXArsCoiVkbEXOA8YH1Hm+uAUwEiYhGtl5O2NFiTJGkKjYVCZj4OvBXYAHwLuCYz746ISyPi7KrZBuAHEXEPcBNwYWb+oKmaJElTi8zOl/n3b2NjYzk+Pj7sMiTpgBIRt2XmWL92fqJZklQYCpKkYsq3pEbEv7L320ih9XbTzMynNVKVJGkopgyFzHzqbBUiSRq+aX14rbo20WGT85l5/8ArkiQNTa1zChFxdkR8F7gX+DLwfeCGBuuSJA1B3RPN7wNOBL6TmSuBlwI3N1aVJGko6obCrupDZXMiYk5m3gQ8v8G6JElDUPecwo6IeArwD8CVEfEQ8HhzZUmShqHukcIaYCfwn4HPA98DXtFUUZKk4ah1pJCZP2mb/XhDtUiShqxWKHR8iG0uMAr8xA+vSdKTS90jhT0+xBYR59C63aYk6UlkRtc+yszrgJcMuBZJ0pDVffnoVW2zc4Axul8TSZJ0AKv7ltT2dxo9TusTzWsGXo0kaajqhsIVmbnHJ5gj4mTgocGXJEkalrrnFP5XzWWSpANYv/spvBg4CVgcEe9qW/U0YKTJwiRJs6/fy0dzgadU7drflvoj4DVNFSVJGo5+N9n5MvDliPhYZt43SzVJkoak7jmFKyJiweRMRCyMiA0N1SRJGpK6obAoM3dMzmTmw8AzmilJkjQsdUPhiYhYPjkTESvww2uS9KRT93MKFwNfiYgvV/O/BqxtpiRJ0rDUvSDe5yNijFYQfAP4LK37K0iSnkTqXvvot4B3AEtphcKJwNfwoniS9KRS95zCO4BfAe7LzFOB1cD2xqqSJA1F3VB4NDMfBYiIQzPz28DRzZUlSRqGuieaJ6rPKVwH3BgRDwPbmitLkjQMtY4UMvOVmbkjM98L/DfgL4Fz+m0XEWdExKaI2BwRF03R7jURkdXJbEnSkNQ9UiiqS1/0FREjwGXAy4AJ4NaIWJ+Z93S0eyrwduCW6dYiSRqsGd2Os6YTgM2ZuSUzHwOupvuNed4HfAB4tMFaJEk1NBkKS4CtbfMT1bIiIlYDyzLzc1N1FBFrI2I8Isa3b/dNT5LUlCZDIbosK5fGiIg5wAeB3+3XUWZenpljmTm2ePHiAZYoSWrXZChMAMva5pey5zuWngocB3wpIr5P6wNx6z3ZLEnD02Qo3AqsioiVETEXOA9YP7kyMx/JzEWZuSIzVwAbgbMzc7zBmiRJU2gsFDLzceCtwAbgW8A1mXl3RFwaEWc39biSpJmb9ltSpyMzrweu71h2SY+2pzRZiySpvyZfPpIkHWAMBUlSYShIkgpDQZJUGAqSpMJQkCQVhoIkqTAUJEmFoSBJKgwFSVJhKEiSCkNBklQYCpKkwlCQJBWGgiSpMBQkSYWhIEkqDAVJUmEoSJIKQ0GSVBgKkqTCUJAkFYaCJKkwFCRJhaEgSSoMBUlSYShIkgpDQZJUGAqSpKLRUIiIMyJiU0RsjoiLuqx/V0TcExF3RsQXIuLIJuuRJE2tsVCIiBHgMuBM4Bjg/Ig4pqPZ7cBYZh4PXAt8oKl6JEn9NXmkcAKwOTO3ZOZjwNXAmvYGmXlTZv60mt0ILG2wHklSH02GwhJga9v8RLWslzcCNzRYjySpj0Ma7Du6LMuuDSMuAMaAX++xfi2wFmD58uWDqk+S1KHJI4UJYFnb/FJgW2ejiDgNuBg4OzN/1q2jzLw8M8cyc2zx4sWNFCtJajYUbgVWRcTKiJgLnAesb28QEauBP6cVCA81WIskqYbGQiEzHwfeCmwAvgVck5l3R8SlEXF21Wwd8BTgMxHxjYhY36M7SdIsaPKcApl5PXB9x7JL2qZPa/LxJUnT4yeaJUmFoSBJKgwFSVJhKEiSCkNBklQYCpKkwlCQJBWGgiSpMBQkSYWhIEkqDAVJUmEoSJIKQ0GSVBgKkqTCUJAkFYaCJKkwFCRJhaEgSSoMBUlSYShIkgpDQZJUGAqSpMJQkCQVhoIkqTAUJEmFoSBJKgwFSVJhKEiSCkNBklQYCpKk4pAmO4+IM4A/AUaAKzLzDzrWHwp8Angh8APgtZn5/abque72B1i3YRPbduzkiAXzuPD0ozln9ZLG+j/1OYu56dvbp3y8OjV16/dzdzzIjp27AFg4f5SXH//M8ljz547w08d2k219jESwO5MlbXU9sGNnz58lYI/tJ+cn+3kymQOMzIFdTwygr4An8ufjtHD+KI/u2s3Ots4XzBvlvWcfW/ZznefN+H0/5KpbtrI7k5EIzn/RMsaOPLzR57MOTpEN/YJHxAjwHeBlwARwK3B+Zt7T1uYtwPGZ+eaIOA94ZWa+dqp+x8bGcnx8fNr1XHf7A7z7r+5i567dZdm80RHe/6rnDuQXqVv/nTofr05NdfrVgWd0TrDu3OcB9N2/k0HTaWROsLttxSCfz3ryiYjbMnOsX7smXz46AdicmVsy8zHgamBNR5s1wMer6WuBl0ZENFHMug2b9vrF27lrN+s2bGqs/06dj1enpjr96sCz64lk3YZNtfZvt0AA9ggEGOzzWQevJkNhCbC1bX6iWta1TWY+DjwC/GJnRxGxNiLGI2J8+/btMypmW4+XSnotH1T/U7WrU9Og6tP+Z9uOnQPfvz5ftK+aDIVu//F3/s9Tpw2ZeXlmjmXm2OLFi2dUzBEL5k1r+aD6n6pdnZoGVZ/2P0csmDfw/evzRfuqyVCYAJa1zS8FtvVqExGHAE8HfthEMReefjTzRkf2WDZvdIQLTz+6sf47dT5enZrq9KsDz+ic4MLTj661f+f0eEF1pGPFIJ/POng1GQq3AqsiYmVEzAXOA9Z3tFkPvL6afg3wxWzozPc5q5fw/lc9lyUL5hHAkgXzBnpSrlv/F5y4fMrHq1NTr34XzBstbRbOH93jsX5h7sheh2Aj1ama9rqm0rn95PxIM6d8hmoOMDqg34TJv9OT47Rw/ijzOjpfMG+Udec+j3NWL6n1vPmfv/F8LjhxeelzJIILTlzO/zj3eY09n3XwauzdRwARcRbwx7TekvrRzPz9iLgUGM/M9RFxGPBJYDWtI4TzMnPLVH3O9N1HknQwq/vuo0Y/p5CZ1wPXdyy7pG36UeDcJmuQJNXnJ5olSYWhIEkqDAVJUmEoSJIKQ0GSVBgKkqTCUJAkFY1+eK0JEbEduG8WH3IR8C+z+Hh17I81wf5ZlzXVtz/WZU319avryMzse/G4Ay4UZltEjNf5FOBs2h9rgv2zLmuqb3+sy5rqG1RdvnwkSSoMBUlSYSj0d/mwC+hif6wJ9s+6rKm+/bEua6pvIHV5TkGSVHikIEkqDAUgIg6PiBsj4rvV94U92u2OiG9UX+vblq+MiFuq7T9d3VSo8Zoi4vkR8bWIuDsi7oyI17at+1hE3NtW7/P3oZYzImJTRGyOiIu6rD+0+rk3V+Owom3du6vlmyLi9JnWMIOa3hUR91Tj8oWIOLJtXdf9OEt1vSEitrc9/m+1rXt9tb+/GxGv79y2wZo+2FbPdyJiR9u6RsYqIj4aEQ9FxDd7rI+I+FBV850R8YK2dU2NU7+aXlfVcmdEfDUinte27vsRcVc1TgO94UuNuk6JiEfa9tMlbeum3PddZeZB/wV8ALiomr4I+MMe7X7cY/k1tG4QBPAR4Ldnoybg2cCqavoI4EFgQTX/MeA1A6hjBPgecBQwF7gDOKajzVuAj1TT5wGfrqaPqdofCqys+hmZpZpOBeZX0789WdNU+3GW6noD8Kddtj0c2FJ9X1hNL5yNmjrav43WDbGaHqtfA14AfLPH+rOAG2jd9O9E4JYmx6lmTSdNPhZw5mRN1fz3gUVDGqtTgM/t676f/PJIoWUN8PFq+uPAOXU3jIgAXgJcO5Pt96WmzPxOZn63mt4GPAT0/XDKNJ0AbM7MLZn5GHB1VVuvWq8FXlqNyxrg6sz8WWbeC2yu+mu8psy8KTN/Ws1upHWP8KbVGateTgduzMwfZubDwI3AGUOo6XzgqgE87pQy8x+Y+n7sa4BPZMtGYEFEPJPmxqlvTZn51eoxYfaeU3XGqpcZPR8NhZZ/k5kPAlTfn9Gj3WERMR4RGyNi8o/0LwI7MvPxan4CGMSNcuvWBEBEnEDrv4HvtS3+/epQ94MRcegM61gCbG2b7/bzlTbVODxCa1zqbNtUTe3eSOu/zknd9uMg1K3r1dV+uTYilk1z26ZqonqJbSXwxbbFTY1VP73qbmqcpqvzOZXA30XEbRGxdgj1vDgi7oiIGyLi2GrZjMaq0dtx7k8i4u+BX+qy6uJpdLM8M7dFxFHAFyPiLuBHXdrVekvXgGqi+g/qk8DrM/OJavG7gf9HKyguB/4rcOl0+p3svsuyzp+vV5s6285E7X4j4gJgDPj1tsV77cfM/F637Ruo62+AqzLzZxHxZlpHWC+puW1TNU06D7g2M3e3LWtqrPqZ7edUbRFxKq1Q+NW2xSdX4/QM4MaI+Hb1H/5s+DqtS1j8OCLOAq4DVjHDsTpojhQy87TMPK7L12eBf67+sE7+gX2oRx/bqu9bgC8Bq2lda2RBREwG7FJg22zVFBFPA/4WeE91mD3Z94PVoffPgP/NzF+2mQCWtc13+/lKm2ocnk7rcLfOtk3VREScRitgz67GAei5Hwehb12Z+YO2Wv4CeGHdbZuqqc15dLx01OBY9dOr7qbGqZaIOB64AliTmT+YXN42Tg8Bf81gXiatJTN/lJk/rqavB0YjYhEzHasmTowcaF/AOvY8qfuBLm0WAodW04uA71KdtAE+w54nmt8ySzXNBb4AvLPLumdW3wP4Y+APZljHIbRO5q3k5yerju1o8zvseaL5mmr6WPY80byFwZxorlPTalovpa2qux9nqa5ntk2/EthYTR8O3FvVt7CaPnw2aqraHU3rZGnMxlhVfa6g98nTl7PnieZ/anKcata0nNZ5sZM6lv8C8NS26a8CZwyqphp1/dLkfqMVRvdX41Zr3+/V3yALP1C/aL3+/YXqSf+FyScZrZcdrqimTwLuqgb2LuCNbdsfBfxT9YT5zOQv0izUdAGwC/hG29fzq3VfrOr8JvAp4Cn7UMtZwHdo/ZG9uFp2Ka3/wAEOq37uzdU4HNW27cXVdpuAMwe4z/rV9PfAP7eNy/p++3GW6no/cHf1+DcBz2nb9j9VY7gZ+M3Zqqmafy8d/zg0OVa0jkgerJ6/E7Rejnkz8OZqfQCXVTXfBYzNwjj1q+kK4OG259R4tfyoaozuqPbtxQN+TvWr661tz6mNtIVWt33f78tPNEuSioPmnIIkqT9DQZJUGAqSpMJQkCQVhoIkqTAUpH0UET+uvh8REdf2afvOiJg/zf5PiYjP7UuNUl2GgtRFRIxMd5vM3JaZr+nT7J3AtEJBmk2Ggg46EbEiIr4dER9vuzDd/Oqa+JdExFeAcyPiWRHx+eoiZ/8YEc+ptl8ZrftY3BoR7+vo95vV9EhE/FF1jf07I+JtEfF2Wpc4vykibqra/fuqr69HxGci4inV8jOqGr8CvGq2x0gHL0NBB6ujgcsz83haFzV8S7X80cz81cy8mtaFBN+WmS8E/gvwZ1WbPwE+nJm/Quuig92spXV5gdXVY1yZmR+ide2ZUzPz1Or6NO8BTsvMFwDjwLsi4jBa10V6BfDv6H7RRKkRB81VUqUOWzPz5mr6U8Dbq+lPA1T/sZ8EfKZ1awigdQ0ngJOBV1fTnwT+sEv/p9G6HtTjAJnZ7Xr4J9K6EdHN1WPMBb4GPAe4N6t7ZUTEp2iFjNQ4Q0EHq87ru0zO/6T6PofWfTJ63ca03/VhomabGzPz/D0Wtm6d6vVnNBS+fKSD1fKIeHE1fT7wlfaVmfkj4N6IOBfKPYMn78l7M62rwQK8rkf/fwe8efKS6hFxeLX8X4GnVtMbgZMj4perNvMj4tnAt4GVEfGstvqkWWEo6GD1LeD1EXEnrcsxf7hLm9cBb4yIyatfTt7K8B3A70TErbTuHdHNFbQuYXxntf1/qJZfDtwQETdl5nZa92y+qqpjI62rpj5K6+Wiv61ONN+3bz+qVJ9XSdVBJyJW0LrR+XFDLkXa73ikIEkqPFKQJBUeKUiSCkNBklQYCpKkwlCQJBWGgiSpMBQkScX/B1HXqgoonmp1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "rescaled_x = rescale(x)\n",
    "beta = estimate_beta(rescaled_x, y)\n",
    "predictions = [predict(x_i, beta) for x_i in rescaled_x]\n",
    "\n",
    "plt.scatter(predictions, y)\n",
    "plt.xlabel(\"predicted\")\n",
    "plt.ylabel(\"actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this approach leads to a couple of issues:\n",
    "\n",
    "1. We'd like our output to be either 0 or 1 to indicate class membership.  It's fine if they are between 1 and 0 because we can interpret these as probabilities.  So an output of 0.25 could mean the member has a 25% chance of being a paid member.  But the outputs of the linear model can be huge positive numbers or even negative numbers, which it's not clear how to interpret.  Indeed, here a lot of our predictions were negative.\n",
    "2. The linear regression model assumed that the errors were uncorrelated with the columns of `x`.  But here, the regresssion coefficient for `experience` is 0.43, indicating that more experience leads to a greater likelihood of a premium account.  This means that our model outputs very large values for people with lots of experience.  But we know that the actual values must be at most 1.  So large outputs correspond to very large negative values of the error term.  Because this is the case, our estimate of `beta` is biased.\n",
    "\n",
    "What we'd like instead is for large positive values of `dot(x_i, beta)` to correspond to probabilities close to 1, and for large negative values to correspnod to probabilities close to 0.  We can accomplish this by applying another function to the result.\n",
    "\n",
    "In the case of logistic regression, we use the *logistic function*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As its input gets large and positive, it gets closer and closer to 1.  As its input gets large and negative, it gets closer and closer to 0.  Additionally, it has the convenient property that its derivative is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_prime(x):\n",
    "    return logistic(x) * (1 - logistic(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we'll make use of shortly.  We'll use this to fit a model:\n",
    "\n",
    "$$ y_i = f(x_i\\beta) + \\epsilon_i $$\n",
    "\n",
    "where $ f $ is the `logistic()` function.  For linear regressions we fit the model by minimizing the sum of squared errors, which ended up choosing the $ \\beta $ that maximized the likelihood of the data being accurate.  Here, the two aren't equivalent.  So, we'll use gradient descent to maximize the likelihood directly.  Given some $ \\beta $, our model says that each $ y_i $ should equal 1 with probability $ f(x_i\\beta) $ and 0 with probability $ 1 - f(x_i\\beta) $.  In particular, the pdf for $ y_i $ can be written as:\n",
    "\n",
    "$$ p(y_i|x_i, \\beta) = f(x_i\\beta)^y_i (1 - f(x_i\\beta))^1-y_i $$\n",
    "\n",
    "since if $ y_i $ is 0, this equals $ 1 - f(x_i\\beta) $, and if $ y_i $ is 1, it equals $ f(x_i\\beta) $.  It's actually simpler to maximize the *log likelihood*:\n",
    "\n",
    "$$ log L(\\beta|x_py_i) = y_i log f(x_i\\beta) + (1 - y_i) log(1 - f(x_i\\beta)) $$\n",
    "\n",
    "Because log is strictly increasing function, any `beta` that maximizes the log likelihood also maximizes the likelihood and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_log_likelihood_i(x_i, y_i, beta):\n",
    "    if y_i == 1:\n",
    "        return math.log(logistic(dot(x_i, beta)))\n",
    "    else:\n",
    "        return math.log(1 - logistic(dot(x_i, beta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume different data points are independent from one another, the overall likelihood is just the product of the individual likelihoods.  Which means the overall log likelihood is the sum of the individual log likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_log_likelihood(x, y, beta):\n",
    "    return sum(logistic_log_likelihood_i(x_i, y_i, beta)\n",
    "               for x_i, y_i in zip(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_log_partial_ij(x_i, y_i, beta, j):\n",
    "    \"\"\"here i is the index of the data point,\n",
    "    j the index of the derivative\"\"\"\n",
    "\n",
    "    return (y_i - logistic(dot(x_i, beta))) * x_i[j]\n",
    "\n",
    "def logistic_log_gradient_i(x_i, y_i, beta):\n",
    "    \"\"\"the gradient of the log likelihood\n",
    "    corresponding to the i-th data point\"\"\"\n",
    "\n",
    "    return [logistic_log_partial_ij(x_i, y_i, beta, j)\n",
    "            for j, _ in enumerate(beta)]\n",
    "\n",
    "def logistic_log_gradient(x, y, beta):\n",
    "    return reduce(vector_add,\n",
    "                  [logistic_log_gradient_i(x_i, y_i, beta)\n",
    "                   for x_i, y_i in zip(x,y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to spolit our data into a training set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_batch [-1.906182482651773, 4.053083869373743, -3.8788953691426906]\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(rescaled_x, y, 0.33)\n",
    "\n",
    "# want to maximize log likelihood on the training data\n",
    "fn = partial(logistic_log_likelihood, x_train, y_train)\n",
    "gradient_fn = partial(logistic_log_gradient, x_train, y_train)\n",
    "\n",
    "# pick a random starting point\n",
    "beta_0 = [1, 1, 1]\n",
    "\n",
    "# and maximize using gradient descent\n",
    "beta_hat = maximize_batch(fn, gradient_fn, beta_0)\n",
    "print(\"beta_batch\", beta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you could use stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta stochastic [-1.9033596650613738, 4.048485018705759, -3.8747571420402442]\n"
     ]
    }
   ],
   "source": [
    "beta_0 = [1, 1, 1]\n",
    "beta_hat = maximize_stochastic(logistic_log_likelihood_i,\n",
    "                               logistic_log_gradient_i,\n",
    "                               x_train, y_train, beta_0)\n",
    "\n",
    "print(\"beta stochastic\", beta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are coefficients for the `rescaled()` data, but we can transform them back to the original data as well:\n",
    "\n",
    "```python\n",
    "beta_hat_unscaled = [7.61, 1.42, -0.000249]\n",
    "```\n",
    "\n",
    "These are not as easy to interpret as linear regression coefficients.  All else being equal, an extra year of experience adds 1.42 to the input of `logistic()`.  All else being equal, and extra $10,000 of salary subtracts 2.49 from the input of `logistic()`.  The impact on the output depends on the other inputs as well.  If `dot(beta, x_i) is already large (corresponding to a probability close to 1), increasing it even by a lot cannot affect the probability very much.  If it's close to 0, increasing it just a little might increase the probability quite a bit.\n",
    "\n",
    "What we can say is that all else being equal, people with more experience are more likely to pay for their accounts, and that people with higher salaries are less likely to pay for accounts.  We haven't used the test data that we held out.  Let's see what happens if we predict *paid account* whenever the probability exceeds 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.9333333333333333\n",
      "recall: 0.8235294117647058\n"
     ]
    }
   ],
   "source": [
    "true_positives = false_positives = true_negatives = false_negatives = 0\n",
    "\n",
    "for x_i, y_i in zip(x_test, y_test):\n",
    "    predict = logistic(dot(beta_hat, x_i))\n",
    "\n",
    "    if y_i == 1 and predict >= 0.5:  # TP: paid and we predict paid\n",
    "        true_positives += 1\n",
    "    elif y_i == 1:                   # FN: paid and we predict unpaid\n",
    "        false_negatives += 1\n",
    "    elif predict >= 0.5:             # FP: unpaid and we predict paid\n",
    "        false_positives += 1\n",
    "    else:                            # TN: unpaid and we predict unpaid\n",
    "        true_negatives += 1\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "print(\"precision: {0}\".format(precision))\n",
    "print(\"recall: {0}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that when we predict *paid account* we are right 93% of the time, and when a user has a paid account we predict *paid account* 82% of the time.  We can also plot the predictions versus the actual, which also shows that the model performs well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XmcXFWd9/HPN50GgiwBE0ayQBBZRJStWRQHQVCQUYg+6BBhBAdhXFAEZUaUQUSdUVHmwRHHQUSQJQFRY8ZBo4+yCEOYNPsajWwJQQlIWBNJwu/545y+VCpVXbc7fbvSle/79epX193O/Z1zb9Wv7rm37lVEYGZmBjCq3QGYmdnaw0nBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KTQASQdJemXg1z2Hkn7D3FIaz1JP5d0TLvjGChJD0k6KL/+rKQLhmGd+0taWPV61kbrYt2dFIZZ7Zt6qETEZRHx9hLrvkjSl+qWfV1EXDuQ9UmaIikkPZf/HpL0mQGG3VYR8Y6IuHioy81t/GJulz9L+pWkHYd6PQAR8S8R8aGSMX2p1XxrI0nXSnpK0vol5+/bN0dXHVunclKwNTE2IjYCjgD+WdLbhnoFI/TN/bXcLpOAx4GLGs00Qus2bCRNAf4aCOCwtgazDnFSWItIOl7S/PwNc5akCTXT3i5pnqSnJX1b0nWSPpSnHSvphvxakv5N0uN53jsl7SzpBOAo4B/zt9j/yvPXdkd05S6JP0h6VtItkia3ijsieoF7gF1r4p0g6UeSFkt6UNInaqaNkXRx/gZ4n6R/rD1EzzH9k6Q7gecljW5R3l6SeiU9I+lPks7J4zeQdKmkJyUtkTRX0l/ladfWtN8oSadLeji32w8kbZqn9X3zPEbSI5KekPS5MtszIl4ALgd2zmWdKemqHNMzwLF53Z/Jbf6kpCslbV5Tt7/LcT1Zv95c3qU1w2+W9D+5rgvyftFsu7faPhfl7XMvsGezOkr6jqSv1437qaRT8ut/kvRo3p/mSTqwTNtlHwDmkJLqKl19OcZv5LZ5WtINksYA1+dZluT6vrFBO61yNCHpg3k/fFbSA5L+YQAxdp6I8N8w/gEPAQc1GP9W4Algd2B94N+B6/O0ccAzwHuA0cBJwHLgQ3n6scAN+fXBwC3AWEDAa4Et87SLgC81iwc4FbgL2CEvuwvwygaxTiF9exudh/cBXgDenYdH5RjOANYDXg08ABycp38FuA7YjPRt+k5gYV1MtwOTgTElyrsJ+Lv8eiNgn/z6H4D/AjYEuoA9gE3ytGtr2u/vgfm53I2AHwOX1NX1uzmWXYC/AK9tsn2LNs5lXQ78Ng+fmbfb1FynMcAnSR98k/J2/09gep5/J+A5YL887RxgRc32OhO4NL/eCngWmAZ0A68Edm203Utun98Cm+dtcHft9qmr737AAkB5eDNgKTCBtB8tACbUtOW2A3ivzAc+mrfbcuCvaqadl7fhxLxt35TbqG97ja6Zt2inJvvv3wDbkvb5t5D25d3ztP2b1b1T/9oewLr2R/Ok8D1St0Pf8Eb5jTCF9I3pppppym+2RknhrcDvSB/Uo+rWscqHQ308wDzg8BJ16HtTLckfAAF8veaDYW/gkbplTgO+n18XH0B5+EOsnhT+vma4VXnXA18AxtXN8/fA/wBvaFCHa2va79fAR2um7ZDbfnRNXSfVTP9f4MgmbXMRsCy3zR+BWeQPwvzhdH3d/PcBB9YMb1mz7jOAGTXTXgG8SOOkcBrwk35iqk0KZbbPITXTTqB5UhDwCLBfHj4e+E1+/RpS99lBQPcA3ydvzu0wLg/fD5ycX4/K+90u/eybpZNCgzJmAifl1/s3q3un/rn7aO0xAXi4byAingOeJH0TmkBKAn3TAmh4RURE/Ab4Fumb1J8knS9pk5IxTAb+MICYx5GS16dJb57uPH5rYELuxlgiaQnwWeCv8vRV6lP3utG4VuUdB2wP3J+7iN6Zx18CzAZmSFok6WuSulndKm2fX4+uKR/SB3yfF3K9m/l6RIyNiFdFxGERUdum9XXdGvhJTb3uA1bmdddv9+dJ+0QjA9l2A90+D9cXUBNTADNIRygA7wcuy9Pmk46EzgQelzRDNV2iLRwD/DIinsjDl/NyF9I4YAMGtq82JekdkuYoddsuAQ7N61gnOSmsPRaR3qwASHoFqQvgUeAxUvdC3zTVDteLiG9GxB7A60gflqf2TWoRwwLSYXRpEbEyIr5B+nb80ZpyHswfjH1/G0fEoXn6KvUhfaCtVnRdXE3Li4jfR8Q0YAvgq8BVkl4REcsj4gsRsROpe+GdpKOuequ0PakrZgXwpwE0RVn122AB8I66um0QEX3bvWgbSRuS9olG+tt2jdbZavvUbpOtWtRpOnCEpK1JRyE/KlYccXlEvJnUvkHaPv3K5wbeB7xF0h8l/RE4GdhF0i6kbtZlTerbaB9/ntSF2OdVNetaP8f7dVL31FjgatIR0DrJSaE9uvNJ0L6/0aRvQh+UtGveUf8FuDkiHgL+G3i9pKl53o9Rs2PXkrSnpL3zN+LnSW+elXnyn0j9x81cAHxR0nZK3iCp2YdQva+QTmZuQOpeeSafZByjdAJ7Z0l9JyyvBE6TtJmkicCJLcrutzxJR0saHxEvkbptAFZKOkDS6yV1kc7JLK9pi1rTgZMlbSNpI1LbXxERK0rWfU18B/hy/kBF0nhJh+dpVwHvzCeQ1wPOovl79jLgIEnvUzox/0pJfSf+67f7QLbPJODj/VUgIm4DFpP2n9kRsSTXZQdJb8378zJSl0+j9q83Nc+3E+nihV1J58Z+C3wgb+cLgXOUTph35RPK6+c4Xqqr7+3AfpK2UrqA4LSaaeuRzkUsBlZIegfQ8vLuTuak0B5Xk94gfX9nRsSvgX8mfWt5jPQt6EiAfAj9XuBrpO6DnYBe0gnPepuQToo+RTrsf5L0LQjSeYudcpfBzAbLnkP6QPgl6UP0e6SToWX8d17n8RGxEngX6c38IOmb3QXApnnes0jdXw8C/4/04deoLkA6GmlR3iHAPZKeA84l9fcvIyXOq3Jd7iOd3L6U1V1I6mq6Ppe/jBYfhEPoXNJ5h19KepZ00nlvgIi4h/QF4HLSPvEUzbsNHyF1e3wK+DPpg3CXPHmV7V6iPb9A2nceJO0Ll5Sox3TSuYPLa8atT/qy8ASp+20LUjdV3w8u72lS1jGk8xuPRMQf+/5I3aJH5S9GnyZdFDE31/erpHNoLwBfBm7M9d0nIn4FXEG6oOEW4Gc17fYs8AnSfv8UqftrVon6dqy+E4M2gkgaRfpwOCoirml3PGtK0kdIH+RvaXcsZus6HymMEJIOljQ2HyJ/ltTnOafNYQ2KpC0l7at0jf4OpG+3P2l3XGaWrrCwkeGNpEPz9YB7gakRsbS9IQ3aeqTr8bchnQOYAXy7rRGZGeDuIzMzq+HuIzMzK4y47qNx48bFlClT2h2GmdmIcssttzwREeNbzTfiksKUKVPo7e1tdxhmZiOKpKa/TK/l7iMzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmhcp+vCbpQtKTrh6PiJ0bTBfpXvKHkh5veGxE3FpVPAMx87ZHOXv2PBYtWcqEsWM49eAdmLrbxHaHNeI1alego9t6sPtSq+X6pj+6ZCldEisjmDjE7TeQ2E+feRfTb17Aygi6JKbtPZkvTX39GpU5XKqOqb/yT595F5fNeaR4XNwr1uviy+9+PVN3m9i2tqrshniS9gOeA37QJCkcSnqQyaGkh4qcGxF7tyq3p6cnqvxF88zbHuW0H9/F0uUvPyBqTHcX//qe17d95x3JGrVr9yiBYPnKl/fBTmrrwe5LrZZrNH0g5Q917KfPvItL5zyyWhlH77PVKolhbXxvVR1Tf+X3Pvznhu3WNUpM22syP7rl0SGNS9ItEdHTar7Kuo8i4nrSE5GaOZyUMCIi5gBjJW1ZVTxlnT173mpvtqXLV3L27HltiqgzNGrX5S/FKgkBOqutB7svtVqu0fSBlF/GQGKffvOChmXUj18b31tVx9Rf+c3abeVLwfSbF7Strdp5TmEi6QHifRbmcauRdIKkXkm9ixcvrjSoRUsaP6Kg2XgrZyDt1yltPdh9qdVyg11+IAYS+8omvQ3149fG91bVMfVXfrN2g+ZtOhxt1c6koAbjGrZERJwfET0R0TN+fMub/K2RCWMbP5K42XgrZyDt1yltPdh9qdVyg11+IAYSe5cavZVXH782vreqjqm/8pu1GzRv0+Foq3YmhYXA5JrhScCiNsVSOPXgHRjT3bXKuDHdXcVJURucRu3aPUp0d62683dSWw92X2q1XKPpAym/jIHEPm3vyauNazR+bXxvVR1Tf+U3a7euUelEfbvaqp23zp4FnChpBulE89MR8Vgb4wEoTuKsbVdIjHTN2rXRuE5p68HuS62Wq51e1dVHA4m972Ryq6uP1sb3VtUx9Vd+37RmVx/1bL15x119NB3YHxgH/An4PNANEBHfyZekfgs4hHRJ6gcjouVlRVVffWRm1onKXn1U2ZFCRExrMT2Aj1W1fjMzGzj/otnMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKxQaVKQdIikeZLmS/pMg+lbSbpG0m2S7pR0aJXxmJlZ/ypLCpK6gPOAdwA7AdMk7VQ32+nAlRGxG3Ak8O2q4jEzs9aqPFLYC5gfEQ9ExIvADODwunkC2CS/3hRYVGE8ZmbWQpVJYSKwoGZ4YR5X60zgaEkLgauBjzcqSNIJknol9S5evLiKWM3MjGqTghqMi7rhacBFETEJOBS4RNJqMUXE+RHRExE948ePryBUMzODapPCQmByzfAkVu8eOg64EiAibgI2AMZVGJOZmfWjyqQwF9hO0jaS1iOdSJ5VN88jwIEAkl5LSgruHzIza5PKkkJErABOBGYD95GuMrpH0lmSDsuzfQo4XtIdwHTg2Iio72IyM7NhMrrKwiPiatIJ5NpxZ9S8vhfYt8oYzMysPP+i2czMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZoWVSkLS9pF9LujsPv0HS6dWHZmZmw63MkcJ3gdOA5QARcSfpectmZtZhyiSFDSPif+vGragiGDMza68ySeEJSdsCASDpCOCxSqMyM7O2GF1ino8B5wM7SnoUeBA4utKozMysLVomhYh4ADhI0iuAURHxbPVhmZlZO7RMCpLGAh8ApgCjJQEQEZ+oNDIzMxt2ZbqPrgbmAHcBL1UbjpmZtVOZpLBBRJxSeSRmZtZ2Za4+ukTS8ZK2lLR531/lkZmZ2bArc6TwInA28DnyZan5/6urCsrMzNqjTFI4BXhNRDxRdTBmZtZeZbqP7gFeqDoQMzNrvzJHCiuB2yVdA/ylb6QvSTUz6zxlksLM/Ddgkg4BzgW6gAsi4isN5nkfcCbpPMUdEfH+wazLzMzWXJlfNF8saT1g+zxqXkQsb7WcpC7gPOBtwEJgrqRZEXFvzTzbke7Aum9EPCVpi8FUwszMhkaZ5ynsD/ye9AH/beB3kvYrUfZewPyIeCAiXgRmAIfXzXM8cF5EPAUQEY8PIHYzMxtiZbqPvgG8PSLmQXroDjAd2KPFchOBBTXDC4G96+bZPpd5I6mL6cyI+EV9QZJOAE4A2GqrrUqEbGZmg1Hm6qPuvoQAEBG/A7pLLKcG46JueDSwHbA/MA24IN9radWFIs6PiJ6I6Bk/fnyJVZuZ2WCUOVLolfQ94JI8fBRwS4nlFgKTa4YnAYsazDMnn6N4UNI8UpKYW6J8MzMbYmWOFD5C+q3CJ4CTgHuBD5dYbi6wnaRt8onqI4FZdfPMBA4AkDSO1J30QLnQzcxsqJU5UhgNnBsR50BxVdH6rRaKiBWSTgRmk84XXBgR90g6C+iNiFl52tsl3Uv6PcSpEfHkIOtiZmZrSBH13fx1M0hzgIMi4rk8vBHwy4h40zDEt5qenp7o7e1tx6rNzEYsSbdERE+r+cp0H23QlxAA8usN1yQ4MzNbO5VJCs9L2r1vQNIewNLqQjIzs3Ypc07hk8APJfVdObQl6aSxmZl1mDJJ4U5gR2AH0m8P7qfcEYaZmY0wZT7cb4qI5RFxd0TclX9TcFPVgZmZ2fBreqQg6VWkW1WMkbQbL/9CeRN8otnMrCP11310MHAs6ZfI59SMfxb4bIUxmZlZmzRNChFxMXCxpP8TET8axpjMzKxNypxo3lnS6+pHRsRZFcRjZmZtVCYpPFfzegPgncB91YRjZmbtVObJa9+oHZb0dVa/sZ2ZmXWAwfzeYEPg1UMdiJmZtV/LIwVJd/Hyw3G6gPGAzyeYmXWgMucU3lnzegXwp4hYUVE8ZmbWRi27jyLiYWAs8C7g3cBOVQdlZmbt0TIpSDoJuAzYIv9dJunjVQdmZmbDr0z30XHA3hHxPICkr5LuffTvVQZmZmbDr8zVRyI9KrPPSl6+D5KZmXWQMkcK3wdulvSTPDwV+F51IZmZWbuU+fHaOZKuBd5MOkL4YETcVnVgZmY2/MocKRARtwK3VhyLmZm1mZ+gZmZmBScFMzMrOCmYmVmhv8dxPsvL9zxaZRIQEbFJZVGZmVlb9PfktY2HMxAzM2u/UlcfAUjagvSQHQAi4pFKIjIzs7Ypc++jwyT9HngQuA54CPh5xXGZmVkblDnR/EVgH+B3EbENcCBwY6VRmZlZW5RJCssj4klglKRREXENsGuZwiUdImmepPmSPtPPfEdICkk9JeM2M7MKlDmnsETSRsD1pNtmP0562E6/JHUB5wFvAxYCcyXNioh76+bbGPgEcPNAgzczs6FV5kjhcGApcDLwC+APpAfutLIXMD8iHoiIF4EZuax6XwS+BiwrFbGZmVWmzJPXno+IlRGxIiIujohv5u6kViYCC2qGF+ZxBUm7AZMj4mf9FSTpBEm9knoXL15cYtVmZjYYZa4+elbSM/lvmaSVkp4pUXajZy4UP4aTNAr4N+BTrQqKiPMjoiciesaPH19i1WZmNhhlbp29yo/YJE0ldQ21shCYXDM8CVhUM7wxsDNwrSSAVwGzJB0WEb0lyjczsyE24HsfRcRM4K0lZp0LbCdpG0nrAUcCs2rKeToixkXElIiYAswBnBDMzNqo5ZGCpPfUDI4Cemh8T6RVRMQKSScCs4Eu4MKIuEfSWUBvRMzqvwQzMxtuZS5Jrb3SaAXpF82NriJaTURcDVxdN+6MJvPuX6ZMMzOrTpmkcEFErPILZkn7Ao9XE5KZmbVLmXMK/15ynJmZjXD9PU/hjcCbgPGSTqmZtAnpHIGZmXWY/rqP1gM2yvPUXpb6DHBElUGZmVl79PeQneuA6yRdFBEPD2NMZmbWJmXOKVwgaWzfgKTNJM2uMCYzM2uTMklhXEQs6RuIiKeALaoLyczM2qVMUnhJ0lZ9A5K2psSP18zMbOQp8zuFzwE3SLouD+8HnFBdSGZm1i5lboj3C0m7kx7JKeDkiHii8sjMzGzYlTlSAFhJ+gXzBsBOkoiI66sLy8zM2qHMDfE+BJxEuvX17aQjhpsod6dUMzMbQcqcaD4J2BN4OCIOAHYD/PgzM7MOVCYpLIuIZQCS1o+I+4Edqg3LzMzaocw5hYX5x2szgV9JeopVn6BmZmYdoszVR+/OL8+UdA2wKfCLSqMyM7O2KHv1EVDcD8nMzDrUgJ/RbGZmnctJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVKk0Kkg6RNE/SfEmfaTD9FEn3SrpT0q8lbV1lPGZm1r/KkoKkLuA84B3ATsA0STvVzXYb0BMRbwCuAr5WVTxmZtZalUcKewHzI+KBiHgRmAEcXjtDRFwTES/kwTnApArjMTOzFqpMChOBBTXDC/O4Zo4Dft5ogqQTJPVK6l28ePEQhmhmZrWqTApqMC4azigdDfQAZzeaHhHnR0RPRPSMHz9+CEM0M7NaA3oc5wAtBCbXDE8CFtXPJOkg4HPAWyLiLxXGY2ZmLVR5pDAX2E7SNpLWA44EZtXOIGk34D+BwyLi8QpjMTOzEipLChGxAjgRmA3cB1wZEfdIOkvSYXm2s4GNgB9Kul3SrCbFmZnZMKiy+4iIuBq4um7cGTWvD6py/WZmNjD+RbOZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVlhdJWFSzoEOBfoAi6IiK/UTV8f+AGwB/Ak8LcR8VCVMc287VFOvuJ2Yg3KEBDAKMH6o0exbPlLbDqmmxdXrOSF5S8BsGH3KNbv7mLJC8uZMHYMB+w4np/d8RhLli4HYLMNu/n8u17H1N0mcvrMu7js5keIHNSG3aN4zx6TuOb+xSxaspQJY8cw5ZVj+J8//Llh3PtuuzmXHf/GAbXB2bPnFWWfevAOTN1tYqllHl2ylC6JlRHF/4kly2i17lZxNZoOlBpXJrahaqu1WafVx4aeItbk47GfgqUu4HfA24CFwFxgWkTcWzPPR4E3RMSHJR0JvDsi/ra/cnt6eqK3t3dQMc287VE+ecXtg1q2Ct1dYq8pm3HjH/68xmWVTQwzb3uU0358F0uXryzGjenu4l/f8/qmHw6NlqnXqoxW6wb6javRst2jBILlK1/eh7u7BAHLX4qG5QzEYNpqbdZp9bGBkXRLRPS0mq/K7qO9gPkR8UBEvAjMAA6vm+dw4OL8+irgQEmqKqCzZ8+rquhBWb4yhiQhAKXLOXv2vNU+3JcuX9lv2zRapl6rMlqtu1VcjaYvfylWSQiQ2rQ2IZSNbaDxjkSdVh+rRpVJYSKwoGZ4YR7XcJ6IWAE8DbyyviBJJ0jqldS7ePHiQQe0aMnSQS/bKZq1QX9tU7bdWs3X37pbxbWm224wyw+mrdZmnVYfq0aVSaHRN/76vqoy8xAR50dET0T0jB8/ftABTRg7ZtDLdopmbdBf25Rtt1bz9bfuVnGt6bYbzPKDaau1WafVx6pRZVJYCEyuGZ4ELGo2j6TRwKbA0PSnNNB3EnJt0d0l9t128yEpq2w5px68A2O6u1YZN6a7q9+2abRMvVZltFp3q7gaTe8epXQOoXZcl9K5hgHGNtB4R6JOq49Vo8qrj+YC20naBngUOBJ4f908s4BjgJuAI4DfRFVnvqE4mbYuX33U1wYDuQKldpk1ufqozLqbTWu2bNlxgzmROpi2Wpt1Wn2sGpVdfQQg6VDg/5IuSb0wIr4s6SygNyJmSdoAuATYjXSEcGREPNBfmWty9ZGZ2bqq7NVHlf5OISKuBq6uG3dGzetlwHurjMHMzMrzL5rNzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs0KlP16rgqTFwMNrWMw44IkhCGckcZ3XHetivV3n1raOiJY3jxtxSWEoSOot88u+TuI6rzvWxXq7zkPH3UdmZlZwUjAzs8K6mhTOb3cAbeA6rzvWxXq7zkNknTynYGZmja2rRwpmZtaAk4KZmRU6OilIOkTSPEnzJX2mwfT1JV2Rp98sacrwRzm0StT5FEn3SrpT0q8lbd2OOIdSqzrXzHeEpJA04i9dLFNnSe/L2/oeSZcPd4xVKLF/byXpGkm35X380HbEOVQkXSjpcUl3N5kuSd/M7XGnpN3XeKUR0ZF/pKe9/QF4NbAecAewU908HwW+k18fCVzR7riHoc4HABvm1x9ZF+qc59sYuB6YA/S0O+5h2M7bAbcBm+XhLdod9zDV+3zgI/n1TsBD7Y57Deu8H7A7cHeT6YcCPyc9JXgf4OY1XWcnHynsBcyPiAci4kVgBnB43TyHAxfn11cBB0oSI1fLOkfENRHxQh6cA0wa5hiHWpntDPBF4GvAsuEMriJl6nw8cF5EPAUQEY8Pc4xVKFPvADbJrzcFFg1jfEMuIq4nPaq4mcOBH0QyBxgracs1WWcnJ4WJwIKa4YV5XMN5ImIF8DTwymGJrhpl6lzrONK3jJGsZZ0l7QZMjoifDWdgFSqznbcHtpd0o6Q5kg4ZtuiqU6beZwJHS1pIehTwx4cntLYZ6Hu+pUqf0dxmjb7x119/W2aekaR0fSQdDfQAb6k0our1W2dJo4B/A44droCGQZntPJrUhbQ/6Wjwt5J2joglFcdWpTL1ngZcFBHfkPRG4JJc75eqD68thvwzrJOPFBYCk2uGJ7H6oWQxj6TRpMPN/g7V1nZl6oykg4DPAYdFxF+GKbaqtKrzxsDOwLWSHiL1u84a4Seby+7bP42I5RHxIDCPlCRGsjL1Pg64EiAibgI2IN04rlOVes8PRCcnhbnAdpK2kbQe6UTyrLp5ZgHH5NdHAL+JfPZmhGpZ59yV8p+khNAJ/cz91jkino6IcRExJSKmkM6jHBYRve0Jd0iU2bdnki4qQNI4UnfSA8Ma5dArU+9HgAMBJL2WlBQWD2uUw2sW8IF8FdI+wNMR8diaFNix3UcRsULSicBs0lULF0bEPZLOAnojYhbwPdLh5XzSEcKR7Yt4zZWs89nARsAP8zn1RyLisLYFvYZK1rmjlKzzbODtku4FVgKnRsST7Yt6zZWs96eA70o6mdSNcuxI/qInaTqpC3BcPk/yeaAbICK+QzpvcigwH3gB+OAar3MEt5eZmQ2xTu4+MjOzAXJSMDOzgpOCmZkVnBTMzKzgpGDTtQiPAAAEWklEQVRmZgUnBesokp7L/ydIuqrFvJ+UtOEAy99fUiW3y+iLfQDzXyTpiAbjeyR9M78+VtK38usPS/pAzfgJQxG3dZaO/Z2CdQ5JXRGxciDLRMQi0g8S+/NJ4FLS9d2VyzdbVNW3XMg/zFvtx3n5uvY+xwJ3M8JvGGdDz0cK1jaSpki6X9LF+V7wV/V9c5f0kKQzJN0AvFfStpJ+IekWSb+VtGOebxtJN0maK+mLdWXfnV93Sfq6pLvyej4u6RPABOAaSdfk+d6ey7pV0g8lbZTHH5LjvAF4T5O6HCvppznGeZI+XxPHfZK+DdwKTJY0Lcdyt6Sv1pXzjbz+X0san8cdn+t3h6Qf1R3dHJTb43eS3pnnb3g0I+lMSZ/ORxc9wGWSbpf0N5J+UjPf2yT9eACb0jqIk4K12w7A+RHxBuAZ0jMu+iyLiDdHxAzSffI/HhF7AJ8Gvp3nORf4j4jYE/hjk3WcAGwD7JbXc1lEfJP0LfmAiDgg3wridOCgiNid9E37FEkbAN8F3gX8NfCqfuqyF3AUsCspkfXdX2kH0u2NdwOWA18F3prn21PS1DzfK4Bb8/qvI/16FeDHEbFnROwC3Ee6v0+fKaSbGv4N8J0cb78i4qpcv6MiYlfSr2Jf25eESL+K/X6rcqwzOSlYuy2IiBvz60uBN9dMuwIgf2N/E+nWHLeT7t3Ud8/4fYHp+fUlTdZxEOlhSisAIqLRTQ/3IT2U5ca8jmOArYEdgQcj4vf5dgmX9lOXX0XEkxGxFPhxTV0ezve6B9gTuDYiFud4LiM9SAXgpb4617XFzvlo4C5S0nldzTqvjIiXIuL3pHsb7dhPfA3lel1CuuX0WOCNjPxbqtsg+ZyCtVv9fVZqh5/P/0cBS/K32jJl1FPJeX4VEdNWGSntWmLZZnH0DT9fM24gD3HqW/4iYGpE3CHpWNK9cFqtc6C+D/wX6SFEP+xLoLbu8ZGCtdtWSve9h3Qv/BvqZ4iIZ4AHJb0XiufS7pIn38jLNzI8qsk6fgl8WOn26EjaPI9/lnRrbUh3T91X0mvyPBtK2h64H9hG0rY1MTbzNkmbSxoDTM2x1bsZeIukcZK6cnnX5WmjePnk+Pt5uS02Bh6T1N2gju+VNCrH92rSLbLLqK1734n5RaQutItKlmEdyEnB2u0+4BhJdwKbA//RZL6jgOMk3QHcw8uPYTwJ+JikuaTnYTRyAemWynfm5d+fx58P/FzSNRGxmHRFzvQcyxxgx4hYRjon8d/5RPPD/dTlBlI3zO3Ajxrdnjvf1vg04BrSM4ZvjYif5snPA6+TdAvpnMNZefw/k5LJr0hJqtY8UlL5OfDhHG8ZF5HOQdyekxikrqwFEXFvyTKsA/kuqdY2kqYAP4uIndscyhrL3To9EXFiu2MZrPx7htsi4nvtjsXax+cUzIx8dPI86XkEtg7zkYKZmRV8TsHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzw/wE5SWo6qe6SNgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = [logistic(dot(beta_hat, x_i)) for x_i in x_test]\n",
    "plt.scatter(predictions, y_test)\n",
    "plt.xlabel(\"predicted probability\")\n",
    "plt.ylabel(\"actual outcome\")\n",
    "plt.title(\"Logistic Regression Predicted vs. Actual\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
